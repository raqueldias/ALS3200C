{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "overfitting_diag.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raqueldias/ALS3200C/blob/main/overfitting_diag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy5inGK6bgCs"
      },
      "source": [
        "# diagnosing model overfitting\n",
        "\n",
        "You can find a supplemental video on this exercise [here](https://youtu.be/K4_G7LUNt3E)\n",
        "\n",
        "Your model is \"overfitting\" your training data when it starts to 'fit' the 'error' in the data, rather than fitting the general 'pattern'. When this happens, the model becomes 'specific' to the *particular* data set you used to train it, and it won't \"generalize\" well to *new* data.\n",
        "\n",
        "We can 'exploit' what overfitting 'looks like' to develop a 'diagnostic' for determining if a model is 'overfit' or not.\n",
        "\n",
        "We just need *two* data sets:\n",
        "\n",
        "* __training data__ to fit the parameters of the model\n",
        "* __validation data__ to 'check' if the fit model 'generalizes' to new data\n",
        "\n",
        "If the model performs roughly *the same* on the training and validation data, then the model is probably *not* overfitting the training data. However, if the model performs *much better* on the *training* data, it is probably overfitting.\n",
        "\n",
        "How do we measure the model's 'performance'?\n",
        "\n",
        "Well, we *already* have the \"loss function\", which tells us how well the model's predictions 'fit' the 'real' response values. We can use the *same* loss function that we use to fit the model to the training data to evaluate the model's performance on the validation data.\n",
        "\n",
        "So, our \"overfitting diagnostic\" procedure will go something like this:\n",
        "\n",
        "1. Partition the data into 2 sub-sets: training and validation\n",
        "2. Fit the model to the training data and evaluate the \"training loss\"\n",
        "3. Evaluate the model's \"validation loss\" using the validation data\n",
        "4. If the validation loss is *much higher* than the training loss, the model is \"overfit\" to the training data (here we assume that *lower* loss is *better*, which it usually is). Otherwise, if the validation loss and the training loss are *about the same*, we can conclude that the model is probably not overfitting the training data.\n",
        "\n",
        "This diagnostic procedure is *widely used* in AI and machine learning, so tensorflow and other libraries have *built in procedures* to help facilitate its use.\n",
        "\n",
        "First, we need to partition our data into training and validation sub-sets. We'll start by simulating some data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0D8HcQsWbcre"
      },
      "source": [
        "import sklearn.datasets\n",
        "\n",
        "x,y = sklearn.datasets.make_regression(n_samples=100,\n",
        "                                       n_features=1,\n",
        "                                       bias=100.0,\n",
        "                                       noise=10.0,\n",
        "                                       random_state=201188)\n",
        "\n",
        "y /= 100.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0lj5Od4h_b6"
      },
      "source": [
        "And we can plot our data, as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vus2673iicDn"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(x,y, marker='o')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh3uU_DqicWk"
      },
      "source": [
        "These are pretty 'noisy' data, but it is what it is. We can't 'decide' what data we'll get, only how we'll analyze it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eFcrqaui_4X"
      },
      "source": [
        "## splitting data into training and validation sub-sets\n",
        "\n",
        "Now let's 'split' or \"partition\" our existing data into training and validation sub-sets. This 'mimics' the procedure of going out and collecting an entirely *new* data set for validation.\n",
        "\n",
        "If this were a *real* data analysis, we would want to be *extremely careful* about how we partition our data into training and validation sub-sets, so we don't 'fool' ourselves into thinking our model is doing well, when in fact it isn't. We want our validation to be a fair and realistic assessment of our model's expected performance on new data.\n",
        "\n",
        "In general, we want our validation data set to have a couple important properties:\n",
        "\n",
        "* We want the validation data to be \"representative\" of the \"entire\" data set. We *don't* want the validation data to *only* represent a narrow range of x- or y-values; we want it to span the *entire range* of both x- and y-values.\n",
        "* We want the validation data to be \"independent\" of the training data. Ideally, the validation data should represent an entirely *new* and independent data sample from the same underlying distribution as the training data. If the validation data samples are *too similar* to the training samples, the validation data and the training data are 'essentially the same', and the validation data no longer represents a good way to test how the model will perform on 'new' data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7KceHpkszXS"
      },
      "source": [
        "For now, we'll just 'randomly' partition our data into training and validation sub-sets, and then do a quick 'check' to see if our random partition is okay. This is probably the most common approach used in AI and machine learning.\n",
        "\n",
        "It's so common that scikit-learn has a function that does it for us!\n",
        "\n",
        "We just need to decide *how much* of the original data we want to set aside for validation. Typically, we'd set aside somewhere between 10% and 40% of the data for validation, based on the size of the original data set. When the data set is 'smaller', we would typically set aside a *larger proportion* of the data for validation (20-40%), whereas a 'larger' data set would only require a *small proportion* of validation data.\n",
        "\n",
        "In our case, we'll decide to use 20% of the data for validation.\n",
        "\n",
        "The scikit-learn function that partitions data into training and validation sub-sets is called \"train_test_split\", and it's in the \"sklearn.model_selection\" sub-module. Just be aware that scikit-learn is using the term, \"test\" to refer to \"validation\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wMsueZBms4A"
      },
      "source": [
        "import sklearn.model_selection\n",
        "\n",
        "train_x, valid_x, train_y, valid_y = sklearn.model_selection.train_test_split(x,\n",
        "                                                                              y,\n",
        "                                                                              test_size=0.2,\n",
        "                                                                              random_state=221882)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNH_r8SomtfE"
      },
      "source": [
        "Here, we supply the \"train_test_split\" function with our *original* explanatory and response variables (x and y, respectively), and we set the\n",
        "\n",
        "    test_size=0.2\n",
        "\n",
        "option to use 20% of the original data for the \"test\" data, which is actually \"*validation*\" data!\n",
        "\n",
        "We also set the \"random_state\" option to some integer value, just so we can reproduce the *same* data split many times.\n",
        "\n",
        "The \"train_test_split\" function returns *4* values, in order:\n",
        "\n",
        "1. training data explanatory variables (train_x, in our example)\n",
        "2. validation data explanatory variables (valid_x)\n",
        "3. training data response variables (train_y)\n",
        "4. validation data response variables (valid_y)\n",
        "\n",
        "We've captured the output of the \"train_test_split\" function call in these four variables, so we can use them later on.\n",
        "\n",
        "Let's plot the training and validation data sub-sets, to see how they look."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZvIBpc5yAF2"
      },
      "source": [
        "plt.scatter(train_x, train_y, marker='o')\n",
        "plt.scatter(valid_x, valid_y, marker='x')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oK1TfUTlyAha"
      },
      "source": [
        "Here, we're plotting the training data as blue circles and the validation data as orange xs.\n",
        "\n",
        "For this particular split, it seems like the validation data represents *most of* the range of both the x- and y-axes, although the orange xs do appear to be a little clustered in the *center* of the graph. Importantly, the orange xs are *not* right on top of blue circles, so the validation data appears relatively *independent* of the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB2W4Xon5ea-"
      },
      "source": [
        "## building the neural network model\n",
        "\n",
        "The neural network model is *exactly the same*, whether you are using validation data or not. So, let's just build a simple linear model for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiB5wmnF5dQi"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(),\n",
        "              loss=tf.keras.losses.MeanSquaredError())\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esThODoR5dqW"
      },
      "source": [
        "done. We used the stochastic gradient descent optimizer (SGD) and mean-squared error loss function, as before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQxtEpKJ8khQ"
      },
      "source": [
        "## packaging data and fitting the model\n",
        "\n",
        "Packaging the data into a Dataset object for tensorflow is also *exactly* the same as before, although now we need *two* different Dataset objects: one will hold the training data, and the other will hold the validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UamUDUrC8yyu"
      },
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(10)\n",
        "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y)).batch(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGtB3L9y8zJS"
      },
      "source": [
        "Should look pretty familiar. As before, we 'batched' our data using a batch size of 10.\n",
        "\n",
        "Let's fit the model.\n",
        "\n",
        "In this case, we need an *additional* option to the \"model.fit\" method call, supplying the *validation* data to the model, which will calculate an *additional* loss score as the model is trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "030Xv4rR9R9O"
      },
      "source": [
        "model.fit(train_data, epochs=100, validation_data=valid_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siCGdPgu9S0J"
      },
      "source": [
        "The *only* new addition to the \"model.fit\" method call is the\n",
        "\n",
        "    validation_data=valid_data\n",
        "\n",
        "option, where we supply the tensorflow model with the packaged validation data set (\"valid_data\"). Tensorflow will handle all the rest!\n",
        "\n",
        "When you fit the model, you will see that tensorflow reports the \"loss:\", just like before, during each epoch of training. This is the model's loss on the *training* data, which we are using to fit the model's parameters.\n",
        "\n",
        "In addition, tensorflow reports the \"val_loss:\", which is the *same* loss function, calculated using the *validation* data set.\n",
        "\n",
        "We can track the \"loss\" to evaluate the model's fit to the training data as it trains, and we can track the \"val_loss\" to evaluate the model's expected loss on *new* validation data.\n",
        "\n",
        "In this particular case, we see that the loss on the *training data* reaches a minimum of around 0.0127 (in my case; your results may vary a little bit). The loss on the *validation data* is very similar (0.0091, in my case) to the training loss, suggesting that the simple linear model is *not* overfitting to the training data.\n",
        "\n",
        "As before, we can plot the fit model against the data, just to check."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZHw_k9JCyM0"
      },
      "source": [
        "y_hat = model.predict(x)\n",
        "\n",
        "plt.scatter(train_x, train_y, marker='o')\n",
        "plt.scatter(valid_x, valid_y, marker='x')\n",
        "plt.scatter(x, y_hat, marker='+')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk65aYI0DFbZ"
      },
      "source": [
        "As before, the blue circles are the training data, and the orange xs are the validation data. The green +s are the model's predictions for the *entire* data set (x)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyQdhdPoCl5A"
      },
      "source": [
        "## evaluating model overfitting end-to-end example\n",
        "\n",
        "Now that we've got the 'basics' of evaluating model overfitting, let's try it out using a more complex model.\n",
        "\n",
        "The following code cell is an end-to-end example, with a few missing pieces you'll need to fill in.\n",
        "\n",
        "We've 'upped the ante' a little bit by simulating 2-dimensional explanatory variables. Notice the\n",
        "\n",
        "    n_features=2\n",
        "\n",
        "option in the \"make_regression\" function call at the beginning?\n",
        "\n",
        "Not a problem; the *only* thing we need to change in our neural network is the\n",
        "\n",
        "    input_shape=[2]\n",
        "\n",
        "option in the *first* network layer, which was \"input_shape=[1]\" in the 1-dimensional case.\n",
        "\n",
        "And of course, we need to plot the data in 3 dimensions, which is a bit more tricky than plotting in 2D. But the plotting isn't really part of this course; it's just there so you can visualize your data and model fit.\n",
        "\n",
        "You should edit the parts of the code cell that say,\n",
        "\n",
        "    FIXME\n",
        "\n",
        "to implement the following required functionality:\n",
        "\n",
        "* partition your simulated data into 80% training data and 20% validation data\n",
        "* package training and validation data into tensorflow Dataset objects with batch size 10\n",
        "* fit your model to the training data using 500 epochs of training; include model evaluation using the validation data\n",
        "\n",
        "Then run the code cell to fit and evaluate your model. Make a note of the final model's loss on the training data and val_loss on the validation data; you will likely need those numbers for the quiz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sLrcbZnI0eU"
      },
      "source": [
        "import sklearn.datasets\n",
        "import sklearn.model_selection\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# simulate data\n",
        "x,y = sklearn.datasets.make_regression(n_samples=100,\n",
        "                                       n_features=2,\n",
        "                                       bias=100.0,\n",
        "                                       noise=10.0,\n",
        "                                       random_state=201188)\n",
        "y /= 100.0\n",
        "\n",
        "# partition your simulated data into 80% training data and 20% validation data\n",
        "train_x, valid_x, train_y, valid_y = sklearn.model_selection.train_test_split(x,\n",
        "                                                                              y,\n",
        "                                                                              test_size=FIXME,\n",
        "                                                                              random_state=221882)\n",
        "\n",
        "# package training and validation data into tensorflow Dataset objects\n",
        "# with batch size 10\n",
        "train_data = FIXME\n",
        "valid_data = FIXME\n",
        "\n",
        "# build model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units=64, activation=tf.keras.activations.relu, input_shape=[2]))\n",
        "model.add(tf.keras.layers.Dense(units=64, activation=tf.keras.activations.relu))\n",
        "model.add(tf.keras.layers.Dense(units=64, activation=tf.keras.activations.relu))\n",
        "model.add(tf.keras.layers.Dense(units=1))\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(),\n",
        "              loss=tf.keras.losses.MeanSquaredError())\n",
        "model.summary()\n",
        "\n",
        "# fit your model to the training data using 500 epochs of training;\n",
        "# include model evaluation using the validation data\n",
        "FIXME\n",
        "\n",
        "# predict using fitted model\n",
        "y_hat = model.predict(x)\n",
        "\n",
        "# plot data and model fit\n",
        "fig = plt.figure(figsize=(12,12))\n",
        "axs = fig.add_subplot(projection='3d')\n",
        "axs.scatter(train_x[:,0], train_x[:,1], train_y, marker='o', s=64)\n",
        "axs.scatter(valid_x[:,0], valid_x[:,1], valid_y, marker='x', s=64)\n",
        "axs.scatter(x[:,0], x[:,1], y_hat, marker='+', s=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYB8BXPtI00G"
      },
      "source": [
        "Is the model's loss on the validation data higher or lower than its loss on the training data?\n",
        "\n",
        "What does that mean for model overfitting?\n",
        "\n",
        "In the 3D plot, the training data are indicated by blue circles. The validation data are indicated by orangs xs, and the model's predicted responses for *all* the data are indicated by green +s.\n",
        "\n",
        "Do you notice a lot of cases in which the green + (model's predicted response) is *exactly* on top of the corresponding blue circle (training data)? These will look like \"&oplus;\". Such a *precise* correspondence between predicted and true response values *on the training data* is a pretty good indication of model overfitting.\n",
        "\n",
        "Also, if you scroll back through the output of the training process, you might notice that the model's loss *on the training data* never actually plateaus; it keeps getting *better and better* as the model trains. Not convinced? Try editing the code cell to train the model for 1000 epochs, and see if the model's loss improves further.\n",
        "\n",
        "While the model's loss on the training data doesn't *have to* plateau, the failure of the model to find a *stable* loss during training is a *potential* 'red flag' that could indicate overfitting or some other problem with the training process. Incidentally, the *presence* of a loss plateau can *also* indicate a potential problem with the training process, especially when the model is not performing well; it can often indicate that the model is 'stuck' in a 'local optimum'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jmb_f_Nrmt5V"
      },
      "source": [
        "## a note on 3-way train-validate-test splits\n",
        "\n",
        "In our data analyses so far, we have been using a *single, pre-determined* neural network to analyze our data. This approach assumes that we already know which specific model we want to use, before collecting *any* data.\n",
        "\n",
        "In many cases, we might want to *try out* a bunch of *different* models, and then pick the *best one* to actually do the data analysis. This procedure is called \"model selection\".\n",
        "\n",
        "The way \"model selection\" typically works in neural network modeling is as follows.\n",
        "\n",
        "* First, you build *many different* neural networks.\n",
        "* Then, each network is fit to the same training data and evaluated for overfitting using the same validation data.\n",
        "* Finally, you use the performance of the models on the validation data to *select* the *best* model. Typically, you just choose the model with the *lowest loss* on the *validation* data as your 'best' model.\n",
        "\n",
        "But then, how do we evaluate the 'best' model's expected performance on *new* data?\n",
        "\n",
        "We *already* used the validation data to select the best model, so we can't *also* use the validation data to evaluate the best model's expected performance on *new* data - the model has *already seen* the validation data!\n",
        "\n",
        "In this case, we need *another* new data sub-set, typically called the \"test data\", in order to evaluate the best model's expected performance.\n",
        "\n",
        "This can be accomplished (and typically is), by partitioning the initial data into *3* sub-sets:\n",
        "\n",
        "* __training data__ - for fitting model parameters\n",
        "* __validation data__ - for evaluating overfitting and choosing the best model\n",
        "* __test data__ - for evaluating the best model's performance on *new* data.\n",
        "\n",
        "The important thing to remember is that, for this procedure to be reliable, you can *only* analyze the test data *__once__* at the very end, using the single model that you have chosen as the 'best' model. After you have calculated your model's loss on the test data, your analysis is *over* and you *can't change anything*. Otherwise, you might be inadvertently using the test data to help build your model, which invalidates the test!\n",
        "\n",
        "The major up-side to this approach is that we can use a 3-way data 'split' to help us 'tune' our neural network architecture to 'fit' the data, allowing us to develop a 'specific' model that is appropriate for the particular data anlaysis we are doing.\n",
        "\n",
        "The major down-side to the 3-way data 'split' is that it *reduces* the amount of data we can use to fit our model. Again, we are *sacrificing* a bit of accuracy in our model's fit. Typically, researchers try to minimize this sacrifice by using the *majority* of the data for model fitting, and only a *small portion* of the data is set aside for validation and testing. For a 'small' data set (100s to 1000s of samples), we might use 60% of the data for training, 20% for validation and 20% for testing. For a 'large' data set (millions of samples or more), we would likely use 80% of the data for training, 10% for validation and 10% for testing.\n",
        "\n",
        "Of course, we still want to make sure that *both* the validation *and* the testing data are representative of the *entire* range of explanatory and response variables, and that all 3 of the data sub-sets are relatively *independent* of one another."
      ]
    }
  ]
}