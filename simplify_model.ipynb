{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simplify_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6nNsB7piGpF"
      },
      "source": [
        "# simplifying the model to mitigate overfitting\n",
        "\n",
        "Overfitting occurs when the model is 'too complex' for the amount of training data you have available. The 'excess complexity' of the model - relative to the data - allows the model to 'tweak' its parameter values so that it 'overfits' the patterns in the data and begins to fit the statistical 'error' in the data set.\n",
        "\n",
        "One way to mitigate model overfitting is to design the model so that it is *just complex enough* to fit the expected *patterns* in the data, removing from the model any 'excess complexity' that could be used to overfit the training data.\n",
        "\n",
        "In the case of probabilistic modeling, the statistical model is specially crafted from expert knowledge of the particular problem domain. The structure and parameters of the model are specified directly from domain-knowledge, so there is a direct link between the model and the expected *processes* that generated the data. In these cases, it's relatively easy to 'tune' the model to match the expected patterns in the data.\n",
        "\n",
        "Unfortunately, neural networks are *not* typically built directly from domain-knowledge. Rather, the majority of neural networks are *general purpose* approaches that can be *applied* to a wide variety of different problems in many different domains. The 'general' nature of neural networks can be a benefit in many cases, because you can use the same type of model in many different problem domains. However, it does make it more difficult to reliably *tune* the neural network model to capture the expected complexity of the data, because there is no direct link between the parameters of the neural network and the 'parameters' of the problem domain.\n",
        "\n",
        "In general, the statistical 'complexity' of a model is related to the number of 'free parameters' in the model (also called \"trainable parameters\" in AI lingo). The more free parameters you have in your model, the higher the model's 'complexity'. Of course, this is a rough correlation, not absolute. But in general, reducing the number of free parameters in a model will typically reduce its complexity and mitigate its potential to 'overfit' training data.\n",
        "\n",
        "A large amount of research effort in neural networks is focused on developing new 'types' of neurons that maintain the exceptional expressive power of the network while simultaneously reducing the number of trainable parameters. Convolution networks, recurrent networks and multi-head attention (aka, \"transformers\") are all types of neurons that were designed with the explicit goal of achieving high expressivity with low parameter counts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pf-Dlf2yN0c"
      },
      "source": [
        "Without resorting to 'fancy' neuron types, we can control the 'complexity' of our neural network by changing its shape (ie, the depth of the network and the widths of its layers).\n",
        "\n",
        "Adding neurons to a network increases the network's parameter count, which can be done by increasing either network depth or layer width. Removing neurons from the network - by removing either entire layers or reducing the width of each layer - will reduce the network's parameter count.\n",
        "\n",
        "Of course, if you remove *too many* neurons from your network, you increase the likelihood that your model will *underfit* the training data, leading to potentially biased and unreliable results. Biases introduced by underfitting can cause very serious problems for statistical analyses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX92QrVt0RRl"
      },
      "source": [
        "In this notebook, we'll start with an 'overspecified' neural network that clearly overfits its training data. We'll then systematically reduce the 'complexity' of the model to reduce overfitting.\n",
        "\n",
        "In the below code cell, we simulate a linear data set of 100 samples with 2 explanatory variables and a single response variable.\n",
        "\n",
        "We split the data set into 60% training and 40% validation data, and we train a complex neural network for 500 epochs using a batch size of 10, stochastic gradient descent optimization and mean squared error loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnTTz2D8iCAW"
      },
      "source": [
        "import sklearn.datasets\n",
        "import sklearn.model_selection\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# simulate data\n",
        "x,y = sklearn.datasets.make_regression(n_samples=100,\n",
        "                                       n_features=2,\n",
        "                                       bias=100.0,\n",
        "                                       noise=10.0,\n",
        "                                       random_state=201188)\n",
        "y /= 100.0\n",
        "\n",
        "# partition into train and validation subsets\n",
        "train_x, valid_x, train_y, valid_y = sklearn.model_selection.train_test_split(x,\n",
        "                                                                              y,\n",
        "                                                                              test_size=40,\n",
        "                                                                              random_state=221882)\n",
        "\n",
        "# package training and validation data into tensorflow Dataset objects\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(10)\n",
        "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y)).batch(10)\n",
        "\n",
        "# build model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units=64, activation=tf.keras.activations.relu, input_shape=[2]))\n",
        "model.add(tf.keras.layers.Dense(units=64, activation=tf.keras.activations.relu))\n",
        "model.add(tf.keras.layers.Dense(units=64, activation=tf.keras.activations.relu))\n",
        "model.add(tf.keras.layers.Dense(units=1))\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(),\n",
        "              loss=tf.keras.losses.MeanSquaredError())\n",
        "model.summary()\n",
        "\n",
        "# fit model\n",
        "model.fit(train_data, epochs=500, validation_data=valid_data)\n",
        "\n",
        "# predict using fitted model\n",
        "y_hat = model.predict(x)\n",
        "\n",
        "# plot data and model fit\n",
        "fig = plt.figure(figsize=(12,12))\n",
        "axs = fig.add_subplot(projection='3d')\n",
        "axs.scatter(train_x[:,0], train_x[:,1], train_y, marker='o', s=64)\n",
        "axs.scatter(valid_x[:,0], valid_x[:,1], valid_y, marker='x', s=64)\n",
        "axs.scatter(x[:,0], x[:,1], y_hat, marker='+', s=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcp2dFQiiFO5"
      },
      "source": [
        "As we've seen before, this model clearly overfits its training data, as evidenced by the final training \"loss\" being much less than the model's loss on the validation data (\"val_loss\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPG_dHZr2Brn"
      },
      "source": [
        "Our current model has 2 hidden layers and 64 neurons (\"units\") in each layer (except the output layer, which must have a single neuron to match the shape of the response variable).\n",
        "\n",
        "We could 'simplify' this model by reducing the number of hidden layers and/or by reducing the number of units in each layer.\n",
        "\n",
        "Let's start by reducing the layer width, as it's the easiest thing to do, operationally.\n",
        "\n",
        "The width of a layer is specified by the \"units\" option, which is just a number. We could manually change each:\n",
        "\n",
        "    units=64\n",
        "\n",
        "option to specify a smaller value, which would decrease the width of that layer.\n",
        "\n",
        "However, we could also introduce a new python 'variable' to hold the layer width, and then use that 'variable' to specify the width of each layer in the network. In that case, we'd only need to change the *variable's* value once, rather than changing it for *each* layer in the network.\n",
        "\n",
        "The following code cell implements the *same* neural network as before, but using a new python variable, that we call \"n_units\".\n",
        "\n",
        "You'll notice that on line 25, we introduce the new \"n_units\" variable, and set its value to 64.\n",
        "\n",
        "    n_units = 64\n",
        "\n",
        "Then, when we add a new Dense layer to the model (lines 29,30,31), we use the \"n_units\" variable to specify the \"units\" option of the layer:\n",
        "\n",
        "    units=n_units\n",
        "\n",
        "Simple.\n",
        "\n",
        "If you run the following code cell as-is, you should get the exact *same* model as in the previous code cell. The model fit will be *slightly* different, of course, due to the 'stochastic' nature of the model fitting process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCzyUdoo5LyG"
      },
      "source": [
        "import sklearn.datasets\n",
        "import sklearn.model_selection\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# simulate data\n",
        "x,y = sklearn.datasets.make_regression(n_samples=100,\n",
        "                                       n_features=2,\n",
        "                                       bias=100.0,\n",
        "                                       noise=10.0,\n",
        "                                       random_state=201188)\n",
        "y /= 100.0\n",
        "\n",
        "# partition into train and validation subsets\n",
        "train_x, valid_x, train_y, valid_y = sklearn.model_selection.train_test_split(x,\n",
        "                                                                              y,\n",
        "                                                                              test_size=40,\n",
        "                                                                              random_state=221882)\n",
        "\n",
        "# package training and validation data into tensorflow Dataset objects\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(10)\n",
        "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y)).batch(10)\n",
        "\n",
        "# specify model shape\n",
        "n_units = 64\n",
        "\n",
        "# build model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units=n_units, activation=tf.keras.activations.relu, input_shape=[2]))\n",
        "model.add(tf.keras.layers.Dense(units=n_units, activation=tf.keras.activations.relu))\n",
        "model.add(tf.keras.layers.Dense(units=n_units, activation=tf.keras.activations.relu))\n",
        "model.add(tf.keras.layers.Dense(units=1))\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(),\n",
        "              loss=tf.keras.losses.MeanSquaredError())\n",
        "model.summary()\n",
        "\n",
        "# fit model\n",
        "model.fit(train_data, epochs=500, validation_data=valid_data)\n",
        "\n",
        "# predict using fitted model\n",
        "y_hat = model.predict(x)\n",
        "\n",
        "# plot data and model fit\n",
        "fig = plt.figure(figsize=(12,12))\n",
        "axs = fig.add_subplot(projection='3d')\n",
        "axs.scatter(train_x[:,0], train_x[:,1], train_y, marker='o', s=64)\n",
        "axs.scatter(valid_x[:,0], valid_x[:,1], valid_y, marker='x', s=64)\n",
        "axs.scatter(x[:,0], x[:,1], y_hat, marker='+', s=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTS4RbAh6YYJ"
      },
      "source": [
        "Now, if you want to change the layer width, you just need to set \"n_units\" to a *different* value *once* in the code, rather than having to do it three times. This makes the code easier to maintain and more readable.\n",
        "\n",
        "Most neural networks have layer widths that are 'powers of 2' (1024, 512, 256, 128, 64, 32, 16, 8, 4, 2, 1). Historically, this can help neural network calculations better 'fit' into the GPU hardware, and sometimes trigger additional hardware speed-ups that make the calculations run faster. Given the rapid pace of GPU hardware development, it's not clear that this 'power of 2 rule' is really that important in practice. \n",
        "\n",
        "Model development should primarily be guided by the goal of producing reliable, accurate results. Only *after* the model is performing well from a statistical perspective should you switch your focus to improving *computational* efficiency.\n",
        "\n",
        "For now, take some time altering the \"n_units\" parameter in the previous code cell to implement networks of different layer widths. Observe the results of increasing or decreasing layer widths on model overfitting in this example. Can you reduce overfitting to the point where val_loss &leq; loss after training? How small must the layer width be to eliminate overfitting?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t4hXapL_j2r"
      },
      "source": [
        "## changing network depth programmatically\n",
        "\n",
        "Making it really easy to change the number of hidden layers in the network takes a little more work (and a python for loop), but we can use a similar approach as we did to change the network's layer width: Create a new python variable that will hold the number of hidden layers in the network, and then use that variable when the model is being built.\n",
        "\n",
        "Of course, we could just copy-and-paste hidden layers into and out of our network, manually, but where's the fun in that?\n",
        "\n",
        "First, we'll define a new python variable called \"n_hidden_layers\", and set it equal to the number of hidden layers currently in our network (which is 2):\n",
        "\n",
        "    n_hidden_layers = 2\n",
        "\n",
        "This is on line 26 in the following code cell.\n",
        "\n",
        "Next, we need to *use* this variable to add the *correct number* of hidden layers to our network.\n",
        "\n",
        "To do this, we introduce a \"for loop\" on line 31 of the following code cell:\n",
        "\n",
        "    for i in range(n_hidden_layers):\n",
        "\n",
        "The for loop specification line ends in a colon \":\", telling python that the following *indented* line(s) should be executed multiple times.\n",
        "\n",
        "How many times? The:\n",
        "\n",
        "    range(n_hidden_layers)\n",
        "\n",
        "part tells python that the for loop should be executed \"n_hidden_layers\" times.\n",
        "\n",
        "The:\n",
        "\n",
        "    for i in ...\n",
        "\n",
        "part is mostly just python syntax for creating a for loop. The \"i\" creates a new variable that can be used inside the for loop's indented code, but we don't use it in this example.\n",
        "\n",
        "The *indented* code following the for loop specifier (line 32 in the following code cell) will be executed *each time* through the for loop. That code adds a new hidden layer to our model:\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(units=n_units, activation=tf.keras.activations.relu))\n",
        "\n",
        "So, each time through the for loop, we add a new Dense layer with \"n_units\" neurons to the Sequential model. We do this \"n_hidden_layers\" times, and n_hidden_layers=2, so we'll have 2 hidden layers in our network.\n",
        "\n",
        "This should create the *same* neural network model that we had in the original code cell, which you can verify by running the following code cell and observing the model's summary output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMWhHyDHBUoP"
      },
      "source": [
        "import sklearn.datasets\n",
        "import sklearn.model_selection\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# simulate data\n",
        "x,y = sklearn.datasets.make_regression(n_samples=100,\n",
        "                                       n_features=2,\n",
        "                                       bias=100.0,\n",
        "                                       noise=10.0,\n",
        "                                       random_state=201188)\n",
        "y /= 100.0\n",
        "\n",
        "# partition into train and validation subsets\n",
        "train_x, valid_x, train_y, valid_y = sklearn.model_selection.train_test_split(x,\n",
        "                                                                              y,\n",
        "                                                                              test_size=40,\n",
        "                                                                              random_state=221882)\n",
        "\n",
        "# package training and validation data into tensorflow Dataset objects\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(10)\n",
        "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y)).batch(10)\n",
        "\n",
        "# specify model shape\n",
        "n_units = 64\n",
        "n_hidden_layers = 2\n",
        "\n",
        "# build model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units=n_units, activation=tf.keras.activations.relu, input_shape=[2]))\n",
        "for i in range(n_hidden_layers):\n",
        "  model.add(tf.keras.layers.Dense(units=n_units, activation=tf.keras.activations.relu))\n",
        "model.add(tf.keras.layers.Dense(units=1))\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(),\n",
        "              loss=tf.keras.losses.MeanSquaredError())\n",
        "model.summary()\n",
        "\n",
        "# fit model\n",
        "model.fit(train_data, epochs=500, validation_data=valid_data)\n",
        "\n",
        "# predict using fitted model\n",
        "y_hat = model.predict(x)\n",
        "\n",
        "# plot data and model fit\n",
        "fig = plt.figure(figsize=(12,12))\n",
        "axs = fig.add_subplot(projection='3d')\n",
        "axs.scatter(train_x[:,0], train_x[:,1], train_y, marker='o', s=64)\n",
        "axs.scatter(valid_x[:,0], valid_x[:,1], valid_y, marker='x', s=64)\n",
        "axs.scatter(x[:,0], x[:,1], y_hat, marker='+', s=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2Ew5KsvEGyp"
      },
      "source": [
        "Now you have 2 simple 'dials' you can tweak to 'dial in' your model's complexity.\n",
        "\n",
        "You can change the \"n_units\" parameter to any integer value &ge;1, which will specify the width of the network's input layer and any hidden layers it might have.\n",
        "\n",
        "And you can change the \"n_hidden_layers\" parameter to any integer value &ge;0, which will specify the number of hidden layers in your network.\n",
        "\n",
        "Go ahead and try some various combinations of values for \"n_units\" and \"n_hidden_layers\" in the previous code cell, and observe the impact of your choices on model overfitting. Does one parameter seem to have more or less of an impact on overfitting than the other? How 'big' can you make your network before it starts to overfit the training data? Is there some *minimum* network size you need to fit these data?\n",
        "\n",
        "'Tuning' a neural network's complexity by trial-and-error can provide you with quite a bit of intuitive insight into how various networks are likely to perform on a given data set. Over time and experience, you will start to develop more of a 'feel' for how to design an initial network for a specific problem. "
      ]
    }
  ]
}
