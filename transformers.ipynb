{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOTww9QxRSP4382EOqR6sLr"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIPkgiTlK9Ph"
      },
      "source": [
        "# analyzing sequence data with transformers\n",
        "\n",
        "In this jupyter notebook, we'll build a 'transformer' network to classify data samples from sequence data.\n",
        "\n",
        "For this exercise, we want to focus on building the network, so we'll just analyze some simulated data. We'll use scikit-learn to simulate sequence data using the make_classification function, like we did before.\n",
        "\n",
        "The following data-simulation code cell should look pretty familiar. Lines 6-9 simulate 38,262 data samples of 128 sequence 'features' or 'items', 32 of which are correlated with the binary class labels. Line 10 splits the simulated data into 80% training and 20% validation sub-sets, and we print the 'shapes' of the resulting data on lines 11 and 12.\n",
        "\n",
        "Lines 15-17 are where things get a bit different.\n",
        "\n",
        "In a 'typical' sequence model, the model itself keeps track of 'where' an item is in the sequence; this is true of both Conv1D and LSTM (or other recurrent network) models. In these cases, we don't need to 'encode' the location of each item in the sequence data.\n",
        "\n",
        "Transformer models are different; they do *not* keep track of the location of each item in the sequence. Rather, transformer models process *each* item in the sequence in *exactly the same way*. So, if the location of each item in the sequence is not encoded in the sequence data, the transformer will *not* make use of data 'locality' when it analyzes the sequence data.\n",
        "\n",
        "Lines 15-17 are where we 'encode' the sequence location information as part of the training and validation data.\n",
        "\n",
        "There are 'fancier' ways to encode sequence location information, but in our case, we're just going to encode the location of each item in the sequence as a floating-point number between -2.0 (the *first* item in the sequence) and +2.0 (the *last* item in the sequence).\n",
        "\n",
        "To do this, we first need to create a vector with evenly-spaced numbers between -2.0 and +2.0, one for each of the items in our sequence. We simulated sequence data with 128 'features', so we need to create a vector with 128 evenly-spaced numbers between -2.0 and +2.0.\n",
        "\n",
        "The numpy function:\n",
        "\n",
        "    np.linspace(...)\n",
        "\n",
        "is used to create a vector of evenly-spaced numbers between some starting value and some ending value. So, we can just specify -2.0 as the starting value, +2.0 as the ending value, and 128 as the number of values to create in the vector:\n",
        "\n",
        "    np.linspace(start=-2.0, stop=+2.0, num=128)\n",
        "\n",
        "This will create a vector that looks something like:\n",
        "\n",
        "    [ -2.00, -1.97, -2.94, ..., 1.94, 1.97, 2.00 ]\n",
        "\n",
        "with some rounding error.\n",
        "\n",
        "Now we want to 'connect' the location vector to our existing sequence data, so that the first item in the sequence gets -2.00 'attached' to it, the second item gets value -1.97, etc.\n",
        "\n",
        "Basically, we have sequence data that looks like:\n",
        "\n",
        "    [ a, b, c, ... ]\n",
        "\n",
        "and location data that looks like:\n",
        "\n",
        "    [ -2.00, -1.97, -1.94, ... ]\n",
        "\n",
        "and we want to 'combine' them to create a new sequence that looks like:\n",
        "\n",
        "    [ [a, -2.00], [b, -1.97], [c, -1.94], ... ]\n",
        "\n",
        "We can use the numpy function \"stack\" to combine the sequence data with the location data, but *first* we need to generate the appropriate number of replicated location vectors, one for each 'row' in the data set.\n",
        "\n",
        "To replicate the location vector, we just call:\n",
        "\n",
        "    np.array([loc] * count)\n",
        "\n",
        "Which will create a new numpy 'array' with the values in 'loc' replicated 'count' number of times. But how many times do we need to 'replicate' the location data?\n",
        "\n",
        "We've already split our sequence data into training (train_x) and validation (valid_x) sub-sets, so we'll need to create *two* different-sized location arrays, one with the same number of 'rows' as the training data, and one with the same number of 'rows' as the validation data. We can get the number of 'rows' of the training data by accessing:\n",
        "\n",
        "    train_x.shape[0]\n",
        "\n",
        "Which extracts the dimensionality of the *first* rank of the train_x.shape tuple, which is the number of 'rows' in the train_x data set.\n",
        "\n",
        "So, to create a location array with the location vector replicated the *same* number of times as there are rows in the train_x data set, we just call:\n",
        "\n",
        "    np.array([loc] * train_x.shape[0])\n",
        "\n",
        "We can then 'stack' this location array onto the existing train_x data set, using the \"np.stack(...)\" function call. We need to make sure we specify that we want the *last* ranks of the arrays (technically, tensors) stacked, so we'll need to set axis=-1:\n",
        "\n",
        "    np.stack( [ array1, array2 ], axis=-1)\n",
        "\n",
        "Filling in the appropriate 'values' for \"array1\" and \"array2\":\n",
        "\n",
        "    np.stack( [ train_x, np.array([loc]*train_x.shape[0]) ], axis=-1)\n",
        "\n",
        "On line 16, we use this code to create a new train_x data set that includes location information. And on line 17, we do the same thing to 'update' the valid_x data set. Now we have training and validation data that includes *both* the original sequence data *and* an encoding of the 'location' of each item in the sequence.\n",
        "\n",
        "You can see that we've increased the dimensionality of the last rank of the training (and validation) data by 1, and we print the validation data, so you can see that each item has both sequence data *and* the location information attached to it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yq4vLWFoIXW7"
      },
      "source": [
        "import numpy as np\n",
        "import sklearn.datasets\n",
        "import sklearn.model_selection\n",
        "\n",
        "# generate sequence data\n",
        "x, y = sklearn.datasets.make_classification(n_samples=38262,\n",
        "                                            n_features=128,\n",
        "                                            n_informative=32,\n",
        "                                            random_state=8792439)\n",
        "train_x, valid_x, train_y, valid_y = sklearn.model_selection.train_test_split(x,y, test_size=0.2, random_state=849691)\n",
        "print(train_x.shape, valid_x.shape)\n",
        "print(train_y.shape, valid_y.shape)\n",
        "\n",
        "# add 'location' to sequence data\n",
        "loc = np.linspace(start=-2.0, stop=+2.0, num=128)\n",
        "train_x = np.stack([ train_x, np.array([loc]*train_x.shape[0]) ], axis=-1)\n",
        "valid_x = np.stack([ valid_x, np.array([loc]*valid_x.shape[0]) ], axis=-1)\n",
        "\n",
        "print(train_x.shape, valid_x.shape)\n",
        "print(valid_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVzKxs3aR9sc"
      },
      "source": [
        "Now we have our sequence data, with location information appropriately encoded.\n",
        "\n",
        "As always, it's pretty easy to 'package' our data for tensorflow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGWSEWWFR-IG"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(32)\n",
        "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y)).batch(32)\n",
        "print(train_data, valid_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyjkR3GRSEs6"
      },
      "source": [
        "## building a transformer\n",
        "\n",
        "Transformer networks are not 'mind-blowingly' complicated, but compared to the very simple feed-forward networks we've been building, transformers are a bit more complex.\n",
        "\n",
        "Let's take a look at the basic architecture of a 'transformer':\n",
        "\n",
        "![transformer2.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2F0XV0E6eW7"
      },
      "source": [
        "Before the input data is sent to the 'transformer' block, we perform a linear projection of the data into a basic 'representational space' of a specified dimension. This is indicated by the red neural-network layer labelled \"linear projection\", which is typically implemented as a Dense neural-network layer. This pointwise linear projection simply projects the data (including location information) into whatever dimensionality we specify as the 'representational space' for our sequence data. The only real 'requirement' is that this representational space must match the representational space used by the multi-head attention system and the feed-forward system used later on in the transformer.\n",
        "\n",
        "It's up to the network designer to 'decide' what internal representation dimensionality to use. For a simple sequence model, we might use a 'small' dimensionality of 4 or 8. For more 'complex' problems like natural language processing, we might use very high-dimensional representations, like 256 or 512. Note that the neural network will 'learn' the pointwise linear projection of the data into this representational space.\n",
        "\n",
        "The 'transformer' building-block has 2 main components, highlighted in green and blue, respectively. \n",
        "\n",
        "The first component (green) includes the multi-head attention layer, but it looks like there are some other 'weird' bits in there that we haven't seen before.\n",
        "\n",
        "First, notice that the output of the linear-projection layer is split into 3 replicated pieces, before sending the data to the multi-head attention layer. This 3-way 'split' is required to use the *same* (projected) sequence data for the queries, keys and values required by multi-head attention.\n",
        "\n",
        "The multi-head attention layer produces a single output, but notice that 'weird' arrow that goes from the linear-projection layer's output *around* the multi-head attention layer! That arrow that *bypasses* the multi-head attention layer is called a \"residual connection\". The residual connection represents a *second* path that the information will travel through the network. The output of the linear-projection layer is sent to the multi-head attention layer, but it is *also* replicated and sent *around* the multi-head attention layer.\n",
        "\n",
        "After multi-head attention produces its output, this output is *combined* with the residual connection via pointwise addition, which is indicated by the pink circle with the \"+\" sign in it.\n",
        "\n",
        "These 'residual' connections are common features of many neural network architectures. They help prevent vanishing-gradient problems in deep networks, and they allow neural-network layers to focus more on calculating 'additional' information that can be 'combined' with the input data, rather than requiring each layer in the network to both 'maintain' the current state of the information *and* calculate additional information that might be relevant for classification.\n",
        "\n",
        "Operationally, the presence of residual connections means that our transformer network is *not* a simple feed-forward network. So, we can no longer use the simple Sequential model in tensorflow to implement our transformer. We'll need to use the more complicated \"functional api\" to implement our transformer. We'll get to that a bit later on.\n",
        "\n",
        "Following the residual connection, the transformer architecture applies \"layer normalization\", which normalizes the output sequences to have mean close to zero and variance close to one. I've never seen a strong justification for using layer normalization in transformers, but layer normalization has been part of the transformer design since the original 2017 paper, although there is [this work](https://arxiv.org/pdf/2002.04745.pdf) suggesting that applying layer normalization *before* multi-head attention might be preferable. In our network, we'll use the 'original' post-residual layer normalization method.\n",
        "\n",
        "Okay, so that takes care of the multi-head attention 'block' in the network (shown in green in the above figure).\n",
        "\n",
        "The second 'block' in the transformer architecture is the \"feed-forward\" block (highlighted in blue), which consists of a \"feed-forward\" layer, with the *same* residual connection and layer normalization as was applied with the multi-head attention block.\n",
        "\n",
        "The \"feed-forward\" layer is simply 2 Dense neural-network layers, with ReLU activation in between. This will be 'easy' to  implement in tensorflow. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynsiCDgpFnO-"
      },
      "source": [
        "## implementing the transformer in tensorflow\n",
        "\n",
        "To implement the transformer in tensorflow, we'll need to abandon the simple Sequential model and use the 'functional api' to create a more complicated model that can have 3-way splits (for input to the multi-head attention layers) and 'residual' connections.\n",
        "\n",
        "The tensorflow 'functional api' is just a programming mechanism for 'connecting' neural-network layers. \"API\" stands for Application Programming Interface, a general term used to describe a 'specification' for how to program something in a computer. Tensorflow uses the term \"functional api\" to describe this particular system, because it effectively uses Layer objects *as if* they were python *functions*.\n",
        "\n",
        "In python, we can 'call' a function by specifying:\n",
        "\n",
        "    function_name(inputs)\n",
        "\n",
        "We provide the name of the function, followed by \"(\", and then list the inputs to the function, followed by \")\". We can also 'capture' the *output* of the function like so:\n",
        "\n",
        "    output = function_name(inputs)\n",
        "\n",
        "The tensorflow 'functional api' uses a similar 'syntax' to connect neural-network Layer objects together:\n",
        "\n",
        "    output_layer = layer_specification()(input_layers)\n",
        "\n",
        "Here, we create a new Layer object using:\n",
        "\n",
        "    layer_specification()\n",
        "\n",
        "At the same time, we set the \"input_layers\" to the newly-created layer, and 'capture' the resulting output in the \"output_layer\" variable. It'll probably make a bit more sense once we get going.\n",
        "\n",
        "To get the network started, we *first* need to create a special Input object that will hold the input to the entire network. This Input object is *not* a Layer, it's a tf.keras.Input object. To create such an object, we just need to specify its shape:\n",
        "\n",
        "    inlayer = tf.keras.Input(shape=XX)\n",
        "\n",
        "The \"shape\" option for the tf.keras.Input object is similar to the \"input_shape\" option we've been using for the first layer in our Sequential models. \n",
        "\n",
        "In our case, our data have shape (128,2), so we'll need to specify:\n",
        "\n",
        "    inlayer = tf.keras.Input(shape=(128,2))\n",
        "\n",
        "to create the Input for our functional model.\n",
        "\n",
        "Next, we need to implement the linear-projection layer, which will 'project' our data up into whatever representational space we want.\n",
        "\n",
        "Let's first define the dimensionality of our representational space:\n",
        "\n",
        "    repdim = 4\n",
        "\n",
        "Then we can use a Dense layer to perform the linear projection:\n",
        "\n",
        "    proj = tf.keras.layers.Dense(units=repdim)(inlayer)\n",
        "\n",
        "Notice that the layer specification is just creating a Dense layer with \"repdim\" units:\n",
        "\n",
        "    tf.keras.layers.Dense(units=repdim)\n",
        "\n",
        "where repdim=4, in our case.\n",
        "\n",
        "But then, we 'connect' this Dense layer to our Input object by appending the:\n",
        "\n",
        "    (inlayer)\n",
        "\n",
        "code to the end of our layer specification, so the new Dense layer will take the \"inlayer\" Input we just created as *input*.\n",
        "\n",
        "Finally, we capture the *output* of our new Dense layer in the variable \"proj\", so we can use it later on as *input* to other layers in the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxV-YWiqI7cd"
      },
      "source": [
        "### the multi-head attention (mha) block\n",
        "\n",
        "Tensorflow implements a multi-head attention layer as a tf.keras.layers.MultiHeadAttention object. When creating the layer, we need to specify the number of attention heads (num_heads) and the dimensionality of the keys (key_dim). In our case, we are using \"repdim\" as the variable to hold the dimensionality of our model's internal data representation, so we can set:\n",
        "\n",
        "    key_dim=repdim\n",
        "\n",
        "We'll also set:\n",
        "\n",
        "    num_heads=1\n",
        "\n",
        "for now, so we'll just be using a *single* attention head.\n",
        "\n",
        "The entire specification for our multi-head attention layer is:\n",
        "\n",
        "    tf.keras.layers.MultiHeadAttention(num_heads=1, key_dim=repdim)\n",
        "\n",
        "But we still need to specify the *inputs* to the MultiHeadAttention layer. Remember that multi-head attention layers require 3 inputs: queries, keys and values. In a 'typical' scenario, we'll use the *same* data for all 3 multi-head attention inputs.\n",
        "\n",
        "In our case, the *output* of the *previous* layer in the network has been 'captured' in the variable \"proj\", which is the output produced from our linear-projection layer. So, we can specify the *input* to our MultiHeadAttention layer like:\n",
        "\n",
        "    (proj,proj,proj)\n",
        "\n",
        "which 'replicates' the \"proj\" output 3 times, producing queries, keys and values that are all the same. Finally, we'll need to 'capture' the *output* from the multi-head attention layer for later use:\n",
        "\n",
        "    mha1 = tf.keras.layers.MultiHeadAttention(num_heads=1, key_dim=repdim)(proj,proj,proj)\n",
        "\n",
        "Now we have the *output* from the MultiHeadAttention layer in \"mha1\", and we have the *input* to the MultiHeadAttention layer in \"proj\". Let's create the \"residual\" connection that *bypasses* the MultiHeadAttention layer.\n",
        "\n",
        "To create the 'residual' connection, we'll use a tf.keras.layers.Add() layer, which simply performs pointwise-addition along a list of tensors. In our case, we want to add the \"proj\" tensor to the \"mha1\" tensor, so we can create the Add layer like:\n",
        "\n",
        "    res1 = tf.keras.layers.Add()([proj,mha1])\n",
        "\n",
        "Notice that we use \"[\" ... \"]\" to create a 'list' containing the tensors \"proj\" and \"mha1\", and then we pass this 'list' as *input* to the Add layer. We capture the *output* of the Add layer in a variable called \"res1\".\n",
        "\n",
        "Finally, we want to use a tf.keras.layers.LayerNormalization layer to 'normalize' the output from the multi-head attention block. We'll pass \"res1\" as the input to the LayerNormalization object, and we'll capture its output in a variable called \"nrm1\":\n",
        "\n",
        "    nrm1 = tf.keras.layers.LayerNormalization()(res1)\n",
        "\n",
        "That's the entire multi-head attention block for our transformer network. For convenience, the code for the entire block is reproduced here:\n",
        "\n",
        "    # set dimensionality of internal data representation\n",
        "    repdim = 4\n",
        "\n",
        "    # input and data projection\n",
        "    inlayer = tf.keras.Input(shape=(128,2))\n",
        "    proj = tf.keras.layers.Dense(units=repdim)(inlayer)\n",
        "\n",
        "    # multi-head attention, residual connection, layer normalization\n",
        "    # aka - MHA block\n",
        "    mha1 = tf.keras.layers.MultiHeadAttention(num_heads=1, key_dim=repdim)(proj,proj,proj)\n",
        "    res1 = tf.keras.layers.Add()([proj, mha1])\n",
        "    nrm1 = tf.keras.layers.LayerNormalization()(res1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYMhz5kIShpb"
      },
      "source": [
        "### the feed-forward block\n",
        "\n",
        "After the multi-head attention 'block', the transformer architecture uses a \"feed-forward block\" that consists of a sequence of 2 pointwise linear projections (separated by ReLU nonlinear activation), followed by a residual connection and layer normalization, just like was used in the multi-head attention block.\n",
        "\n",
        "We already have the *output* of the multi-head attention block stored in the \"nrm1\" variable.\n",
        "\n",
        "We need to connect this as *input* to the first of two Dense layers, which will use ReLU nonlinear activation:\n",
        "\n",
        "    ffa1 = tf.keras.layers.Dense(units=repdim, activation=tf.keras.activations.relu)(nrm1)\n",
        "\n",
        "Notice that we use the *same* internal data representation dimensionality as before, by setting \"units=repdim\" in the Dense layer.\n",
        "\n",
        "Next, we need to create the second Dense layer and connect it to the first:\n",
        "\n",
        "    ffb1 = tf.keras.layers.Dense(units=repdim)(ffa1)\n",
        "\n",
        "No nonlinear activation, this time.\n",
        "\n",
        "Next, create the residual connection bypassing *both* Dense layers:\n",
        "\n",
        "    res2 = tf.keras.layers.Add()([nrm1,ffb1])\n",
        "\n",
        "And send the results through a LayerNormalization:\n",
        "\n",
        "    nrm2 = tf.keras.layers.LayerNormalization()(res2)\n",
        "\n",
        "Now we have the *output* of the entire feed-forward block stored in \"nrm2\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jexJ6xPzS2Br"
      },
      "source": [
        "### building the transformer model\n",
        "\n",
        "Our transformer model still needs an output layer, which will classify sequences using binary class labels. This will be a familiar Flatten layer, followed by a Dense layer with a single unit and sigmoid activation. \n",
        "\n",
        "The *output* from the previous feed-forward block is \"nrm2\", which we'll use as *input* to the Flatten layer. In functional api:\n",
        "\n",
        "    flt = tf.keras.layers.Flatten()(nrm2)\n",
        "    outlayer = tf.keras.layers.Dense(units=1, activation=tf.keras.activations.sigmoid)(flt)\n",
        "\n",
        "Now we have our complete transformer network specified using the functional api.\n",
        "\n",
        "The tensorflow functional api specifies the input-output *connections* within our transformer model, but we still need to actually *build* the model in tensorflow.\n",
        "\n",
        "We'll use a tf.keras.Model object to build our transformer model.\n",
        "\n",
        "Using the functional api, we just need to specify the \"inputs\" and the \"outputs\" of the Model object to build our model.\n",
        "\n",
        "In our case, the *input* to our entire model is the Input object stored in \"inlayer\". And the *output* from our entire model is the final classification or 'output' layer, \"outlayer\". So, we just need to do:\n",
        "\n",
        "    model = tf.keras.Model(inputs=inlayer, outputs=outlayer)\n",
        "\n",
        "to build our transformer model.\n",
        "\n",
        "The following code cell combines all the code snippets to build the transformer model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wh16u-JVSET2"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "## build model using functional api\n",
        "repdim = 4  # set internal data representation dimensionality\n",
        "\n",
        "# input and linear projection\n",
        "inlayer = tf.keras.Input(shape=(128,2))\n",
        "proj = tf.keras.layers.Dense(units=repdim)(inlayer)\n",
        "# multi-head attention block\n",
        "mha1 = tf.keras.layers.MultiHeadAttention(num_heads=1, key_dim=repdim)(proj,proj,proj)\n",
        "res1 = tf.keras.layers.Add()([proj, mha1])\n",
        "nrm1 = tf.keras.layers.LayerNormalization()(res1)\n",
        "# feed-forward block\n",
        "ffa1 = tf.keras.layers.Dense(units=repdim, activation=tf.keras.activations.relu)(nrm1)\n",
        "ffb1 = tf.keras.layers.Dense(units=repdim)(ffa1)\n",
        "res2 = tf.keras.layers.Add()([nrm1, ffb1])\n",
        "nrm2 = tf.keras.layers.LayerNormalization()(res2)\n",
        "# classification block\n",
        "flt = tf.keras.layers.Flatten()(nrm2)\n",
        "outlayer = tf.keras.layers.Dense(units=1, activation=tf.keras.activations.sigmoid)(flt)\n",
        "\n",
        "model = tf.keras.Model(inputs=inlayer, outputs=outlayer)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOepZBuCUbWv"
      },
      "source": [
        "Because this model is *not* Sequential, there is a column on the right of the summary that reports the *inputs* to each layer in the network. Notice, for example, that there are *three* inputs to the multi-head attention layer; they are also all the same. Similarly, you'll see *two* inputs to each of the Add layers, representing the 'residual' connections.\n",
        "\n",
        "Let's take a look at the parameter counts in this transformer network.\n",
        "\n",
        "There are 661 total trainable parameters, 513 of which are found in the output or 'decision' layer. So, there are only 148 trainable parameters in the 'transformer' part of the network.\n",
        "\n",
        "Layer normalization requires 8 parameters, two for *each* of the dimensions in the network's internal data representation. One of these parameters 'centers' the data, and the other 'scales' it.\n",
        "\n",
        "The Add layers implementing residual connections don't require any trainable parameters; they just perform pointwise addition.\n",
        "\n",
        "Each of the two Dense layers making up the feed-forward part of the transformer network requires 20 trainable parameters. Let's walk through where these parameters are coming from. The internal data representation has 4 dimensions, so there are 4 input weights to each 'unit' in the Dense layer. Plus 1 bias term for each 'unit'. To maintain the internal data representation dimensionality, we've specified 4 'units' in each Dense layer. So, there are (4+1)*4=20 trainable parameters in each of the Dense layers making up the feed-forward part of the network.\n",
        "\n",
        "That just leaves the multi-head attention layer, which has 80 trainable parameters. Multi-head attention needs to perform *independent* pointwise linear projections of the queries, keys and values. We've already seen from our analysis of the Dense layers in the feed-forward part of the network that *each* of these will require 20 trainable parameters. So that's 60. Where do the remaining 20 trainable parameters come from?\n",
        "\n",
        "Multi-head attention layers need to 'combine' the outputs from *each* of the attention 'heads' to produce the layer's final output. The multi-head attention layer does this by first *concatenating* the outputs from *all* the attention heads, and then performing a final *pointwise linear projection* of these results back *down* into the dimensionality of the internal data representation. Our internal data representation is in 4 dimensions, and we are only using a single attention head in this example. So, our multi-head attention layer calculates *another* pointwise linear projection from 4 dimensions into 4 dimensions. We already know this requires 20 trainable parameters, so that's where the 'missing' parameters in our multi-head attention layer are coming from."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze2XbfNBguRr"
      },
      "source": [
        "If we take a second to look at our transformer network, we see *quite a few* places in which *pointwise linear projections* are calculated:\n",
        "\n",
        "* First, we project our input data into a high-dimensional representational space using a pointwise linear projection.\n",
        "* Next, we perform *three* independent pointwise linear projections to calculate the queries, keys and values as input to the multi-head attention later. In tensorflow, these projections are 'hidden' within the MultiHeadAttention object.\n",
        "* Then, we perform *another* pointwise linear projection to calculate the *output* of the multi-head attention layer, also 'hidden' within the tensorflow MultiHeadAttention object.\n",
        "* Finally, the feed-forward part of the transformer calculates a sequence of *two* different pointwise linear projections, with ReLU activation in between.\n",
        "\n",
        "In contrast to what we observe in a typical feed-forward neural network - in which there are *a lot* of nonlinear activations between network layers - in the transformer architecture, there are relatively more *pointwise linear* transformations, with relatively *few* typical nonlinear activations.\n",
        "\n",
        "Rather, in the transformer architecture, the 'complex' matrix operations performed *inside* the multi-head attention layer provide the *majority* of the 'nonlinearity' in the overall network. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWPPuAjxWp62"
      },
      "source": [
        "## testing the transformer network\n",
        "\n",
        "The following code cell is an end-to-end working transformer network analysis of sequence data, including simulating the data, adding location information, and packaging the data as tensorflow Dataset objects. The transformer network creation is exactly the same as before. We compile the model using BinaryCrossentropy loss and the Adam optimizer, and we fit for 20 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q89wxN8JUbyI"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import sklearn.datasets\n",
        "import sklearn.model_selection\n",
        "\n",
        "## simulate and package data\n",
        "# simulate and split\n",
        "x, y = sklearn.datasets.make_classification(n_samples=38262,\n",
        "                                            n_features=128,\n",
        "                                            n_informative=32,\n",
        "                                            random_state=8792439)\n",
        "train_x, valid_x, train_y, valid_y = sklearn.model_selection.train_test_split(x,y, test_size=0.2, random_state=849691)\n",
        "# add 'location' to sequence data\n",
        "loc = np.linspace(start=-2.0, stop=+2.0, num=128)\n",
        "train_x = np.stack([ train_x, np.array([loc]*train_x.shape[0]) ], axis=-1)\n",
        "valid_x = np.stack([ valid_x, np.array([loc]*valid_x.shape[0]) ], axis=-1)\n",
        "# package in tensorflow Datasets\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(32)\n",
        "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y)).batch(32)\n",
        "\n",
        "\n",
        "## build model using functional api\n",
        "repdim = 4  # set internal data representation dimensionality\n",
        "\n",
        "# input and linear projection\n",
        "inlayer = tf.keras.Input(shape=(128,2))\n",
        "proj = tf.keras.layers.Dense(units=repdim)(inlayer)\n",
        "# multi-head attention block\n",
        "mha1 = tf.keras.layers.MultiHeadAttention(num_heads=1, key_dim=repdim)(proj,proj,proj)\n",
        "res1 = tf.keras.layers.Add()([proj, mha1])\n",
        "nrm1 = tf.keras.layers.LayerNormalization()(res1)\n",
        "# feed-forward block\n",
        "ffa1 = tf.keras.layers.Dense(units=repdim, activation=tf.keras.activations.relu)(nrm1)\n",
        "ffb1 = tf.keras.layers.Dense(units=repdim)(ffa1)\n",
        "res2 = tf.keras.layers.Add()([nrm1, ffb1])\n",
        "nrm2 = tf.keras.layers.LayerNormalization()(res2)\n",
        "# classification block\n",
        "flt = tf.keras.layers.Flatten()(nrm2)\n",
        "outlayer = tf.keras.layers.Dense(units=1, activation=tf.keras.activations.sigmoid)(flt)\n",
        "\n",
        "model = tf.keras.Model(inputs=inlayer, outputs=outlayer)\n",
        "model.summary()\n",
        "\n",
        "## compile model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "## fit model\n",
        "model.fit(train_data, epochs=20, validation_data=valid_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ainAILAKUgrX"
      },
      "source": [
        "After about 20 epochs of training, my transformer network was able to achieve approximately 0.89 accuracy on both the training and validation data, indicating that about 89% of the data samples are being correctly classified by this model.\n",
        "\n",
        "Try altering the previous code cell to encode the data using an internal dimensionality of 8, and use 2 attention heads in your multi-head attention layer.\n",
        "\n",
        "Then try re-running the previous code cell with the new transformer network. Keep track of the number of parameters in your model, as well as the final accuracy scores and the number of seconds required for each epoch of training with GPU acceleration turned on."
      ]
    }
  ]
}
