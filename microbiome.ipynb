{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "microbiome.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPqX2cPAgjgs6Qtam0rqA3J"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mc5xkI2hXrwh"
      },
      "source": [
        "# microbial community disease risk prediction\n",
        "\n",
        "In this case study, we'll develop a neural network to predict disease risk from microbial community sequence data.\n",
        "\n",
        "We have 16S rDNA sequence data from 16,344 samples, roughly half of which are from individuals who have been diagnosed with type 1 diabetes (aka, \"cases\"), and half of which are from individuals who do not have type 1 diabetes (\"controls\").\n",
        "\n",
        "The data are available in a github repository as a comma-separated values (.csv) file. So, we can use the pandas library to download the sequence data and associated disease-state labels to a pandas dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alDR3GTbXqOw"
      },
      "source": [
        "import pandas\n",
        "dataframe = pandas.read_csv('https://raw.githubusercontent.com/bryankolaczkowski/ALS3200C/main/mbiome.data.csv')\n",
        "dataframe.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlKLWh5rXq9g"
      },
      "source": [
        "There are 256 \"DTA\" columns, labelled DTA0, ..., DTA255. Each of these DTA columns represents a particular bacterial \"species\" found in the samples. The 'relative abundance' of each species (column) in each sample (row) is reported. Relative abundance values have been center log-ratio transformed, which is a common method used to 'normalize' microbial relative abundance data.\n",
        "\n",
        "In a typical analysis of 16S rDNA sequence data, the 'relative abundance' of each sequence in the sample is given as the *number* of sequence reads matching that sequence in the sample. One *could* divide each sequence count by the total number of counts in that sample, which would produce a typical 'relative abundance' value between 0.0 (not found in the sample) and 1.0 (the *only* sequence found in the sample).\n",
        "\n",
        "However, it's more common to perform some sort of log-ratio transform of the sequence count data. Log-ratio transforms have a couple of advantages over the 'frequency transform' above. First, putting numbers on a log scale often makes them more 'normally distributed', which typically provides a better fit to the assumptions of most statistical models. Second, the log scale can be 'centered' at zero, with positive and negative values indicating deviations from the 'average' value of zero; this 'centering' often leads to better results from machine-learning and neural-network models.\n",
        "\n",
        "The center log-ratio transform is simple to calculate and is commonly used for microbial community sequence projects. One simply divides each sequence's count by the *geometric* mean of the total counts over all sequences in the sample, and then takes the logarithm of this 'ratio'.\n",
        "\n",
        "These data have already been center log-ratio transformed, and you can see that the values typically range between about +2.5 and -2.5.\n",
        "\n",
        "The final column in the data file, labelled \"LBL0\" is the 'disease state' indicator (the label we'd like to predict), with 0 indicating a 'control' individual with no type 1 diabetes diagnosis, and 1 indicating a 'case' individual who has been diagnosed with type 1 diabetes.\n",
        "\n",
        "Our goal is to predict the LBL0 classification, given the microbial sequence information in columns DTA0, ..., DTA255.\n",
        "\n",
        "First, let's split our data into training and validation subsets, and extract the explanatory variables and labels.\n",
        "\n",
        "Much of the following code cell should look familiar. Given the pandas dataframe, we first sample 80% of the data for training, and leave the remaining 20% for validation.\n",
        "\n",
        "Next, we extract the columns starting with \"DTA\" as the explanatory variables. In this case, we need to 'expand' the data dimension, so we can model these data using a tensorflow sequence model (like a Conv1D or LSTM model).\n",
        "\n",
        "Finally, we extract the LBL0 entries as our binary class labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVTUJdVQYDkv"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# create train-validate split\n",
        "train_dataframe = dataframe.sample(frac=0.8, random_state=2100963)\n",
        "valid_dataframe = dataframe.drop(train_dataframe.index)\n",
        "print(train_dataframe.shape, valid_dataframe.shape, dataframe.shape)\n",
        "\n",
        "# extract explanatory variables\n",
        "dta_ids = [ x for x in dataframe.columns if x.find('DTA') == 0 ]\n",
        "train_x = np.expand_dims(train_dataframe[dta_ids].to_numpy(), axis=-1)\n",
        "valid_x = np.expand_dims(valid_dataframe[dta_ids].to_numpy(), axis=-1)\n",
        "print(train_x.shape, valid_x.shape)\n",
        "\n",
        "# extract labels\n",
        "train_y = train_dataframe['LBL0'].to_numpy()\n",
        "valid_y = valid_dataframe['LBL0'].to_numpy()\n",
        "print(train_y.shape, valid_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8CPzypUY4TL"
      },
      "source": [
        "We can see that there are 16,344 total samples in our dataframe. We've extracted 13,057 for training and 3,269 for validation.\n",
        "\n",
        "After expanding the data dimension, we have explanatory variables of shape (256,1), a one-dimensional sequence of 256 bacterial 'species'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYabR-E7fsni"
      },
      "source": [
        "## simple linear model\n",
        "\n",
        "Now that we have some training and validation data, we just need to build a classifier to predict disease risk from the data.\n",
        "\n",
        "First, we'll start with a simple linear model implemented using a single Dense neuron with sigmoid output, as this is a binary classification problem.\n",
        "\n",
        "In previous cases, we were able to send the data directly to the Dense layer. But in this case, because we have 'expanded' the last dimension of the data - in order to fit tensorflow's sequence models - we need to 'collapse' that dimension back down, so it can be properly analyzed by the Dense layer.\n",
        "\n",
        "It's pretty easy to do this; we just need to use a Flatten layer to 'flatten' the 'expanded' data back down to a simple vector. And, because the Flatten layer is just like any other tensorflow Layer object, we can use it as the *first* layer in our network, provided we set the input_shape option.\n",
        "\n",
        "The following code cell implements a simple linear classifier for our disease-risk prediction problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldKzbLpyXIAW"
      },
      "source": [
        "import pandas\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# download data\n",
        "dataframe = pandas.read_csv('https://raw.githubusercontent.com/bryankolaczkowski/ALS3200C/main/mbiome.data.csv')\n",
        "\n",
        "# create train-validate split\n",
        "train_dataframe = dataframe.sample(frac=0.8, random_state=2100963)\n",
        "valid_dataframe = dataframe.drop(train_dataframe.index)\n",
        "\n",
        "# extract explanatory variables\n",
        "dta_ids = [ x for x in dataframe.columns if x.find('DTA') == 0 ]\n",
        "train_x = np.expand_dims(train_dataframe[dta_ids].to_numpy(), axis=-1)\n",
        "valid_x = np.expand_dims(valid_dataframe[dta_ids].to_numpy(), axis=-1)\n",
        "\n",
        "# extract labels\n",
        "train_y = train_dataframe['LBL0'].to_numpy()\n",
        "valid_y = valid_dataframe['LBL0'].to_numpy()\n",
        "\n",
        "# package data into tensorflow dataset\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(32)\n",
        "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y)).batch(32)\n",
        "\n",
        "# build model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=(256,1)))\n",
        "model.add(tf.keras.layers.Dense(units=1, activation=tf.keras.activations.sigmoid))\n",
        "model.summary()\n",
        "\n",
        "# compile model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# fit model\n",
        "model.fit(train_data, epochs=20, validation_data=valid_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAKPoof9XyDd"
      },
      "source": [
        "The simple linear model has only 257 trainable parameters (256 input weights, plus the single bias term). But, it does pretty well at classifying our data, given its simplicity.\n",
        "\n",
        "After 20 epochs of training, my linear model achieved about 80% classification accuracy on the training data, and about 78% classification accuracy on the validation data.\n",
        "\n",
        "That's a pretty reasonable 'baseline', and it suggests that the *majority* of information required for disease-risk prediction is available in a simple linear model.\n",
        "\n",
        "Let's see if we can do better using slightly more complex approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqMA5a8V_eMI"
      },
      "source": [
        "## convolution model\n",
        "\n",
        "The next code cell is an end-to-end example using a simple nonlinear convolution network for disease-risk prediction.\n",
        "\n",
        "We're using 16 convolution 'filters' of size (3,) in our one-layer network. We've also indicated 'same' padding, so the output of the convolution layer will be the *same* length as the data sequence (256 entries).\n",
        "\n",
        "We decided to use hyperbolic tangent (tanh) nonlinear activation in this convolution layer. Although this activation is a bit unusual for modern-day convolutions (which more commonly use ReLU activation), it matches the default activation used by recurrent neural networks like LSTMs, so we can more directly compare the results from this convolution network with recurrent network models.\n",
        "\n",
        "This model will have 4161 trainable parameters, quite a few more than the simple linear model, so we'll train it for a bit longer (50 epochs, in this case)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93rSQ41XYduw"
      },
      "source": [
        "import pandas\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# download data\n",
        "dataframe = pandas.read_csv('https://raw.githubusercontent.com/bryankolaczkowski/ALS3200C/main/mbiome.data.csv')\n",
        "\n",
        "# create train-validate split\n",
        "train_dataframe = dataframe.sample(frac=0.8, random_state=2100963)\n",
        "valid_dataframe = dataframe.drop(train_dataframe.index)\n",
        "\n",
        "# extract explanatory variables\n",
        "dta_ids = [ x for x in dataframe.columns if x.find('DTA') == 0 ]\n",
        "train_x = np.expand_dims(train_dataframe[dta_ids].to_numpy(), axis=-1)\n",
        "valid_x = np.expand_dims(valid_dataframe[dta_ids].to_numpy(), axis=-1)\n",
        "\n",
        "# extract labels\n",
        "train_y = train_dataframe['LBL0'].to_numpy()\n",
        "valid_y = valid_dataframe['LBL0'].to_numpy()\n",
        "\n",
        "# package data into tensorflow dataset\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(32)\n",
        "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y)).batch(32)\n",
        "\n",
        "# build model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Conv1D(filters=16,\n",
        "                                 kernel_size=(3,),\n",
        "                                 activation=tf.keras.activations.tanh,\n",
        "                                 padding='same',\n",
        "                                 input_shape=(256,1)))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(units=1, activation=tf.keras.activations.sigmoid))\n",
        "model.summary()\n",
        "\n",
        "# compile model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# fit model\n",
        "model.fit(train_data, epochs=50, validation_data=valid_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7xlS_XXYeLG"
      },
      "source": [
        "Keep track of the time needed to train each epoch, as well as the final accuracy and val_accuracy values, for the associated quiz."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4ZyqiR5BWOx"
      },
      "source": [
        "## LSTM model\n",
        "\n",
        "Finally, we'll try using an LSTM model to analyze the sequence data.\n",
        "\n",
        "Implement a single-layer LSTM model in the following code cell, using 16 LSTM 'units', and make sure you set return_sequences=True in your model. The rest of the model should use the same Dense decision layer as before.\n",
        "\n",
        "Remember that LSTM networks can take much longer to train!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijrxhUsmZXtB"
      },
      "source": [
        "import pandas\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# download data\n",
        "dataframe = pandas.read_csv('https://raw.githubusercontent.com/bryankolaczkowski/ALS3200C/main/mbiome.data.csv')\n",
        "\n",
        "# create train-validate split\n",
        "train_dataframe = dataframe.sample(frac=0.8, random_state=2100963)\n",
        "valid_dataframe = dataframe.drop(train_dataframe.index)\n",
        "\n",
        "# extract explanatory variables\n",
        "dta_ids = [ x for x in dataframe.columns if x.find('DTA') == 0 ]\n",
        "train_x = np.expand_dims(train_dataframe[dta_ids].to_numpy(), axis=-1)\n",
        "valid_x = np.expand_dims(valid_dataframe[dta_ids].to_numpy(), axis=-1)\n",
        "\n",
        "# extract labels\n",
        "train_y = train_dataframe['LBL0'].to_numpy()\n",
        "valid_y = valid_dataframe['LBL0'].to_numpy()\n",
        "\n",
        "# package data into tensorflow dataset\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(32)\n",
        "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y)).batch(32)\n",
        "\n",
        "# build model\n",
        "model = tf.keras.models.Sequential()\n",
        "FIXME - implement LSTM model\n",
        "model.summary()\n",
        "\n",
        "# compile model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# fit model\n",
        "model.fit(train_data, epochs=50, validation_data=valid_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbKTMcfrZYHh"
      },
      "source": [
        "Make sure you keep track of the training time per epoch, and the final accuracy and loss values, for the associated quiz."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kj0H4SDANM9"
      },
      "source": [
        "Now that you've tried some 'baseline' models, including a simple linear model and some one-layer convolution and LSTM models, you might want to try 'playing around' with your models, to see if you can improve accuracy without causing too much overfitting.\n",
        "\n",
        "What happens to the model's accuracy when you increase the number of filters (for convolution networks) or units (for LSTM networks)?\n",
        "\n",
        "What happens if you add more layers to the networks?\n",
        "\n",
        "Does training for a longer number of epochs improve accuracy?\n",
        "\n",
        "When you increase the size of your networks, at what point do you start to see overfitting? Can you 'fix' overfitting with Dropout layers or regularization?\n",
        "\n",
        "How accurate can you make your model, without too much overfitting?"
      ]
    }
  ]
}
