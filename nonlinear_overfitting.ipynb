{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nonlinear_overfitting.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPhWJaQRUFinPYU3eMauBcL"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKIuu-j7xTQ-"
      },
      "source": [
        "# introduction to model over-fitting\n",
        "\n",
        "Once you know how to systematically increase the depth and width of your neural network, you can very easily build *very* large networks with only a few lines of actual code.\n",
        "\n",
        "For example, the next code cell creates a neural network with over 33 *million* trainable parameters, in just a few seconds!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEiAUsO7xSm4"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "nw_width = 512\n",
        "nw_hdpth = 128\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units=nw_width, activation=tf.keras.activations.relu, input_shape=[1]))\n",
        "for i in range(nw_hdpth):\n",
        "  model.add(tf.keras.layers.Dense(units=nw_width, activation=tf.keras.activations.relu))\n",
        "model.add(tf.keras.layers.Dense(units=1))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoWF6jSq0jcF"
      },
      "source": [
        "Given that it is *so* easy to build *very large* neural networks, you might be wondering why we don't just analyze *all data* with *very large* neural networks!\n",
        "\n",
        "Well, if you look at the neural-network literature, it may seem like we actually *do* analyze *all* data with *very large* neural networks. The 'size' (measured in number of trainable parameters) of state-of-the-art neural networks has been increasing *dramatically* over recent years, and it seems that - in many complex problem domains - larger networks often result in *better* inferences.\n",
        "\n",
        "There are some *technical* problems with naive implementations of *very deep* nerual networks - like the [vanishing gradient](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) problem - but these problems have largely been solved by 'fancy' network architectures like [residual networks](https://en.wikipedia.org/wiki/Residual_neural_network).\n",
        "\n",
        "Of course, large networks must perform a *large number* of calculations, and calculations take time. Fortunately, the 'forward pass' through a neural network is *very fast*, even for large networks. Backpropagation during training can take a bit longer, but hardware acceleration using GPUs or other 'specialized' hardware has essentially solved this problem. I could probably train a 50-million-parameter network on my laptop (which has an NVIDIA GPU) in a few days, and a big computer with multiple GPUs could probably train it in under 5 minutes!\n",
        "\n",
        "Big networks also consume more memory, which could limit their use on 'small' devices like 'phones' and appliances (did you know both apple and android phones have dedicated hardware devoted to running neural networks?). However, cloud-based computing allows you to make predictions 'online', using any hardware available on the cloud; only the data and results need to be sent over the network.\n",
        "\n",
        "Simple optimization routines like stochastic gradient descent *can* get 'stuck' when used to train large networks, failing to find a good approximation of the loss function's *global optimum*. But there are *lots* of 'fancier' optimization methods that don't suffer as much from these problems.\n",
        "\n",
        "So, from a *computational* perspective, there doesn't seem to be any *strong* argument in favor of using *simpler* neural networks, at least not in general."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltfdlZKL8o6s"
      },
      "source": [
        "## a 'statistical' argument against very large neural networks\n",
        "\n",
        "Given enough time and computational resources, we could probably fit *any* arbitrarily complex neural network to *any* data set. But is it a 'good idea' to build 'arbitrarily complex' statistical models?\n",
        "\n",
        "Later on in the course, we'll spend a lot of time looking at \"overfitting\" with complex neural networks.\n",
        "\n",
        "In this notebook, I'd like to go through a short exercise that might get you thinking about some of the problems that could arise when the model becomes 'too complex' for the data.\n",
        "\n",
        "It will also give you a chance to practice building nonlinear neural networks!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnjUhLRe942n"
      },
      "source": [
        "## back to 'linear' data\n",
        "\n",
        "To illustrate \"overfitting\", let's go back to considering some *simple* data simulated according to a linear model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2diAjSx-EHN"
      },
      "source": [
        "import sklearn.datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x,y = sklearn.datasets.make_regression(n_samples=50,\n",
        "                                       n_features=1,\n",
        "                                       bias=0.0,\n",
        "                                       noise=30.0,\n",
        "                                       random_state=602951)\n",
        "y = y / 100.0\n",
        "\n",
        "plt.scatter(x,y, marker='o')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kht8RoYP-Egj"
      },
      "source": [
        "This should look pretty familiar by now. We just simulated 50 1-dimensional data points along a diagonal line. We scaled the y-values, so they fall roughly between -1.5 and +1.5, but that's just for convenience."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDD8euYu_LEk"
      },
      "source": [
        "Of course, we can fit a simple linear model to these data, and check the fit. This should also look fairly familar by now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaAHt5BX_TZC"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(),\n",
        "              loss=tf.keras.losses.MeanSquaredError())\n",
        "model.summary()\n",
        "\n",
        "data = tf.data.Dataset.from_tensor_slices((x,y)).batch(10)\n",
        "model.fit(data, epochs=100)\n",
        "\n",
        "y_hat = model.predict(x)\n",
        "\n",
        "plt.scatter(x,y, marker='o')\n",
        "plt.scatter(x,y_hat, marker='+')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXgV1gRR_TuE"
      },
      "source": [
        "We get a pretty nice, expected linear fit (orange +) through the center of the simulated data (blue dots).\n",
        "\n",
        "Incidentally, if you'd like to see the 'best fit' values for the slope and bias of your liner model, you can:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D61H2e2ApPr"
      },
      "source": [
        "print(model.trainable_variables)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u3swcvNApft"
      },
      "source": [
        "This will print the 'raw' tensorflow Variables associated with your neural network. For this 'small' network, there are only 2 trainable variables:\n",
        "\n",
        "* the slope - called the \"kernel\" of the Dense neuron\n",
        "* the y-intercept - called the \"bias\" of the neuron\n",
        "\n",
        "If you look at the printed output, you should see a part that reads\n",
        "\n",
        "    'dense_XXX/kernel:0'\n",
        "\n",
        "where \"XXX\" is an auto-generated number that tensorflow assigns to 'name' the dense layer of your network. After that, you see the specification of the \"kernel\" (ie, slope) variable:\n",
        "\n",
        "    shape=(1, 1) dtype=float32, numpy=array([[0.49990593]], dtype=float32)\n",
        "\n",
        "In my case, the *value* of the best-fit slope was 0.49990593; this is pretty close to 0.5.\n",
        "\n",
        "Similarly, we can find the 'best fit' value for the bias variable by looking for\n",
        "\n",
        "    'dense_XXX/bias:0'\n",
        "\n",
        "and reading its value:\n",
        "\n",
        "    shape=(1,) dtype=float32, numpy=array([0.01880047], dtype=float32)\n",
        "\n",
        "In my case, the 'best fit' y-intercept value was 0.01880047; this is pretty close to 0.0.\n",
        "\n",
        "The\n",
        "\n",
        "    print(model.trainable_variables)\n",
        "\n",
        "statement will print the current values of *all* the trainable parameters in your model. For simple models like this one, we can manually read through the output and interpret the 'meaning' of the values, but for very large networks, printing all the Variables to the screen is unlikely to be very helpful!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORr_MyXHCoqD"
      },
      "source": [
        "## fitting a nonlinear model to linear data\n",
        "\n",
        "Okay, but we've fit linear models, before. Let's fit a *nonlinear* model to these data, and see how 'good' it is.\n",
        "\n",
        "The following code cell is an end-to-end example, including data simulation, model fitting and plotting the model fit.\n",
        "\n",
        "The only thing missing is the model!\n",
        "\n",
        "In this case, we simulate a small sample of data from a linear model, and we fit a linear model to the data, for comparison.\n",
        "\n",
        "Your job is to build a *nonlinear* neural network model, which should be implemented between the\n",
        "\n",
        "    ## -- BEG BUILD MODEL\n",
        "\n",
        "and\n",
        "\n",
        "    ## -- END BUILD MODEL\n",
        "\n",
        "comments in the code cell. Replace the \"FIXME\" text with your model implementation. Notice that an \"empty\" Sequential model has already been created; you just need to add layers to the empty model, to build your nonlinear neural network.\n",
        "\n",
        "Before you take the quiz, you should *not* change *anything* in the code cell, except for your model implementation. *After* you've completed the quiz, feel free to play around with different model implementations, if you'd like. And remember that you can always grab a 'fresh' version of this notebook from the course link, if you need to.\n",
        "\n",
        "Build your nonlinear model according to the following specification:\n",
        "\n",
        "* Your model should have 4 hidden layers (ie, the network's total depth should be 6).\n",
        "* Use Dense layers for *all* layers in your network.\n",
        "* Use ReLU activations on *all* layers of your network *except* the output layer, which should have *linear* activation.\n",
        "* The width of *all* layers in your model *except* the output layer should be 64.\n",
        "\n",
        "That's it. Please implement your model and run the code cell to confirm it works and check your model's fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEozgEuFDPYp"
      },
      "source": [
        "import sklearn.datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# simulate linear data\n",
        "x,y = sklearn.datasets.make_regression(n_samples=50,\n",
        "                                       n_features=1,\n",
        "                                       bias=0.0,\n",
        "                                       noise=30.0,\n",
        "                                       random_state=602951)\n",
        "y = y / 100.0\n",
        "data = tf.data.Dataset.from_tensor_slices((x,y)).batch(10)\n",
        "\n",
        "# fit linear model, so we can compare later\n",
        "lm = tf.keras.Sequential()\n",
        "lm.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "lm.compile(optimizer=tf.keras.optimizers.SGD(),\n",
        "              loss=tf.keras.losses.MeanSquaredError())\n",
        "lm.fit(data, epochs=100, verbose=0)\n",
        "lm_yhat = lm.predict(x)\n",
        "\n",
        "# build and fit nonlinear model\n",
        "model = tf.keras.Sequential()\n",
        "## -- BEG BUILD MODEL\n",
        "FIXME\n",
        "## -- END BUILD MODEL\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),\n",
        "              loss=tf.keras.losses.MeanSquaredError())\n",
        "model.summary()\n",
        "\n",
        "model.fit(data, epochs=500)\n",
        "\n",
        "y_hat = model.predict(x)\n",
        "\n",
        "# plot results\n",
        "plt.scatter(x,y, marker='o')\n",
        "plt.scatter(x,lm_yhat, marker='+')\n",
        "plt.scatter(x,y_hat, marker='s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYWmSFMUZO73"
      },
      "source": [
        "In the plot, the blue circles indicate the 'true' simulated data. The orange + indicates the best-fit *linear* model, and the green squares indicate the fit to the training data for your *nonlinear* neural network.\n",
        "\n",
        "Do you think the nonlinear model is a 'better' fit?"
      ]
    }
  ]
}