{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_regularization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6nNsB7piGpF"
      },
      "source": [
        "# model regularization to mitigate overfitting\n",
        "\n",
        "Model overfitting can often be associated with very 'extreme' values of fitted model parameters. When model parameter values become highly positive or highly negative, the model can tend to react very 'strongly' to particular inputs, potentially causing lack of generalizability to new data.\n",
        "\n",
        "Statisticians have long used model \"regularization\" to help control overfitting by 'penalizing' extreme values of model parameters. Regularization is extremely popular in the 'machine learning' world, forming the foundation for methods like \"lasso\" and \"ridge\" regression. Model regularization is also a very common technique used in neural network modeling.\n",
        "\n",
        "The two most commonly-used model regularization penalties are:\n",
        "\n",
        "*  L1 regularization\n",
        "*  L2 regularization\n",
        "\n",
        "The names are actually more 'informative' than you might think. Both methods apply an increasing 'penalty' on the model's loss function as the sum of the model's parameters gets farther away from zero, either positively or negatively.\n",
        "\n",
        "L1 regularization's penalty scales as the *absolute value* of the model's parameter value, so the L1 term scales *linearly* with 'increasing' model parameter values.\n",
        "\n",
        "L2 regularization's penalty scales as the *square* of the model's parameter value. You can remember it because L2 has a \"2\" in it, for 'squared'.\n",
        "\n",
        "Practically, L2 regularization applies a much *stronger* penalty as the model's parameter value gets very large (either positively or negatively), while applying a much *weaker* penalty when the parameter value is less than 1. For this reason, L2 regularization tends to produce models with many small parameter values and few values that are zero, compared to L1 regularization, which tends to produce 'sparse' models in which many parameter values are zero.\n",
        "\n",
        "In neural networks, model regularization can be applied in any of 3 different ways:\n",
        "\n",
        "*  kernel regularization - penalizes the neuron's *weights*\n",
        "*  bias regularization - penalizes the neuron's *bias term*\n",
        "*  activity regularization - penalizes the neuron's *output*\n",
        "\n",
        "As a neural network designer, you can apply any or all of these regularization penalties to any or all of the layers in your network. \n",
        "\n",
        "Probably the most common approach is to apply kernel regularization (penalizing the neuron's weights) to all layers in the network *except* the output layer, which is typically *not* regularized.\n",
        "\n",
        "To use model regularization, you need to specify the \"regularization_factor\", which is a number multiplied by the regularization term to control the *strength* of regularization. Mathematically:\n",
        "\n",
        "    total_loss = loss + regularization_factor * regularization_term\n",
        "\n",
        "you calculate the \"regularization_term\", based on the model's parameter values and the type of regularization being used (typically L1 or L2). Multiply the regularization_term by the user-specified regularization_factor, and then add this to the model's \"loss\" to get the total_loss.\n",
        "\n",
        "Commonly-used regularization factors are typically in the range of 0.01 or 0.001, but they can vary greatly, depending on the model being developed. Typically you will 'tweak' the regularization factor until you've reduced overfitting to an 'acceptable' amount."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX92QrVt0RRl"
      },
      "source": [
        "In this notebook you will implement L1 and L2 kernel regularization and observe its impact on model overfitting in an example problem.\n",
        "\n",
        "In the below code cell, we simulate a linear data set of 100 samples with 2 explanatory variables and a single response variable.\n",
        "\n",
        "We split the data set into 60% training and 40% validation data, and we train a complex neural network for 500 epochs using a batch size of 10, stochastic gradient descent optimization and mean squared error loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnTTz2D8iCAW"
      },
      "source": [
        "import sklearn.datasets\n",
        "import sklearn.model_selection\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# simulate data\n",
        "x,y = sklearn.datasets.make_regression(n_samples=100,\n",
        "                                       n_features=2,\n",
        "                                       bias=100.0,\n",
        "                                       noise=10.0,\n",
        "                                       random_state=201188)\n",
        "y /= 100.0\n",
        "\n",
        "# partition into train and validation subsets\n",
        "train_x, valid_x, train_y, valid_y = sklearn.model_selection.train_test_split(x,\n",
        "                                                                              y,\n",
        "                                                                              test_size=40,\n",
        "                                                                              random_state=221882)\n",
        "\n",
        "# package training and validation data into tensorflow Dataset objects\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(10)\n",
        "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y)).batch(10)\n",
        "\n",
        "# build model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units=64, activation=tf.keras.activations.relu, input_shape=[2]))\n",
        "model.add(tf.keras.layers.Dense(units=64, activation=tf.keras.activations.relu))\n",
        "model.add(tf.keras.layers.Dense(units=64, activation=tf.keras.activations.relu))\n",
        "model.add(tf.keras.layers.Dense(units=1))\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(),\n",
        "              loss=tf.keras.losses.MeanSquaredError())\n",
        "model.summary()\n",
        "\n",
        "# fit model\n",
        "model.fit(train_data, epochs=500, validation_data=valid_data)\n",
        "\n",
        "# predict using fitted model\n",
        "y_hat = model.predict(x)\n",
        "\n",
        "# plot data and model fit\n",
        "fig = plt.figure(figsize=(12,12))\n",
        "axs = fig.add_subplot(projection='3d')\n",
        "axs.scatter(train_x[:,0], train_x[:,1], train_y, marker='o', s=64)\n",
        "axs.scatter(valid_x[:,0], valid_x[:,1], valid_y, marker='x', s=64)\n",
        "axs.scatter(x[:,0], x[:,1], y_hat, marker='+', s=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUhJVvhu5eny"
      },
      "source": [
        "In tensorflow, model regularization is implemented by a tf.keras.regularizers.Regularizer object. The *type* of regularization you use is determined by the *type* of regularizer:\n",
        "\n",
        "*   tf.keras.regularizers.L1 - implements L1 regularization\n",
        "*   tf.keras.regularizers.L2 - implements L2 regularization\n",
        "\n",
        "There's even a tf.keras.regularizers.L1L2 regularizer, which implements a combination of *both* L1 and L2 regularization. This is probably overly-complex for most situations.\n",
        "\n",
        "Let's implement L1 regularization in our model.\n",
        "\n",
        "We're going to apply L1 regularization to *every* layer in our model, *except* the output layer. And we'll penalize the model's *weights* using 'kernel' regularization.\n",
        "\n",
        "To implement this, we'll begin by creating a python variable to hold the regularization factor, so we can modify it easily later on, based on our observations of how regularization impacts model overfitting.\n",
        "\n",
        "In line 25 of the following code cell, we define:\n",
        "\n",
        "    reg_fact = 0.01\n",
        "\n",
        "This value is a pretty common starting point; it also happens to be the default in tensorflow.\n",
        "\n",
        "To set the regularization factor of the L1 regularizer object, we need to set the \"l1\" option when we create the regularizer object:\n",
        "\n",
        "    tf.keras.regularizers.L1(l1=reg_fact)\n",
        "\n",
        "And, we need to add the regularizer object to *each* layer in the network (except the output layer!), by setting the \"kernel_regularizer\" option when we create the Layer:\n",
        "\n",
        "    kernel_regularizer=tf.keras.regularizers.L1(l1=reg_fact)\n",
        "\n",
        "You should see this on lines 29,30,31 of the following code cell.\n",
        "\n",
        "Go ahead and run the following code cell as-is, to observe the impact of L1 regularization on model fitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKsFbLLs8ELO"
      },
      "source": [
        "import sklearn.datasets\n",
        "import sklearn.model_selection\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# simulate data\n",
        "x,y = sklearn.datasets.make_regression(n_samples=100,\n",
        "                                       n_features=2,\n",
        "                                       bias=100.0,\n",
        "                                       noise=10.0,\n",
        "                                       random_state=201188)\n",
        "y /= 100.0\n",
        "\n",
        "# partition into train and validation subsets\n",
        "train_x, valid_x, train_y, valid_y = sklearn.model_selection.train_test_split(x,\n",
        "                                                                              y,\n",
        "                                                                              test_size=40,\n",
        "                                                                              random_state=221882)\n",
        "\n",
        "# package training and validation data into tensorflow Dataset objects\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(10)\n",
        "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y)).batch(10)\n",
        "\n",
        "# set regularization factor\n",
        "reg_fact = 0.01\n",
        "\n",
        "# build model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units=64, activation=tf.keras.activations.relu, kernel_regularizer=tf.keras.regularizers.L1(l1=reg_fact), input_shape=[2]))\n",
        "model.add(tf.keras.layers.Dense(units=64, activation=tf.keras.activations.relu, kernel_regularizer=tf.keras.regularizers.L1(l1=reg_fact)))\n",
        "model.add(tf.keras.layers.Dense(units=64, activation=tf.keras.activations.relu, kernel_regularizer=tf.keras.regularizers.L1(l1=reg_fact)))\n",
        "model.add(tf.keras.layers.Dense(units=1))\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(),\n",
        "              loss=tf.keras.losses.MeanSquaredError())\n",
        "model.summary()\n",
        "\n",
        "# fit model\n",
        "model.fit(train_data, epochs=500, validation_data=valid_data)\n",
        "\n",
        "# predict using fitted model\n",
        "y_hat = model.predict(x)\n",
        "\n",
        "# plot data and model fit\n",
        "fig = plt.figure(figsize=(12,12))\n",
        "axs = fig.add_subplot(projection='3d')\n",
        "axs.scatter(train_x[:,0], train_x[:,1], train_y, marker='o', s=64)\n",
        "axs.scatter(valid_x[:,0], valid_x[:,1], valid_y, marker='x', s=64)\n",
        "axs.scatter(x[:,0], x[:,1], y_hat, marker='+', s=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYuM5EeK9bOq"
      },
      "source": [
        "It's important to note that, with regularization, the \"loss\" and \"val_loss\" reported by tensorflow during training *includes* the regularization penalty, so you *can't* directly compare the loss and val_loss values between models with/without regularization, or between models using different *types* or *strengths* of regularization!\n",
        "\n",
        "The loss and val_loss will *always* be larger with regularization than without.\n",
        "\n",
        "Well, what do you think; did L1 regularization reduce model overfitting in this example? Was the model's final loss value 'close to' the final val_loss?\n",
        "\n",
        "Take some time to try out different values for \"reg_fact\". How small can you make the regularization factor, while still mitigating model overfitting? \n",
        "\n",
        "In many cases, you don't need much regularization to prevent a model from overfitting, especially if the model has a large number of parameters and you apply regularization penalties to most of them.\n",
        "\n",
        "Of course, you can 'turn off' regularization by setting:\n",
        "\n",
        "    reg_fact = 0.0\n",
        "\n",
        "in the previous code cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZGWkFgS_LQs"
      },
      "source": [
        "## implementing L2 kernel regularization\n",
        "\n",
        "Now let's try L2 regularization.\n",
        "\n",
        "Tensorflow L2 regularization is implemented by a tf.keras.regularizers.L2 object, so we'll need to change L1 to L2 in order to apply L2 regularization to the neural network's weights.\n",
        "\n",
        "We'll also have to change the:\n",
        "\n",
        "    l1=reg_fact\n",
        "\n",
        "option to:\n",
        "\n",
        "    l2=reg_fact\n",
        "\n",
        "for the L2 regularizer. This is an unfortunate design choice made by tensorflow; it would have been better to use the *same* option name for *both* L1 and L2 regularization objects, but it is what it is.\n",
        "\n",
        "Modify the \"FIXME\" portions of the following code cell to use L2 regularization, and then run it to see how L2 regularization helps mitigate model overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCODKl0PAiVh"
      },
      "source": [
        "import sklearn.datasets\n",
        "import sklearn.model_selection\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# simulate data\n",
        "x,y = sklearn.datasets.make_regression(n_samples=100,\n",
        "                                       n_features=2,\n",
        "                                       bias=100.0,\n",
        "                                       noise=10.0,\n",
        "                                       random_state=201188)\n",
        "y /= 100.0\n",
        "\n",
        "# partition into train and validation subsets\n",
        "train_x, valid_x, train_y, valid_y = sklearn.model_selection.train_test_split(x,\n",
        "                                                                              y,\n",
        "                                                                              test_size=40,\n",
        "                                                                              random_state=221882)\n",
        "\n",
        "# package training and validation data into tensorflow Dataset objects\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(10)\n",
        "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y)).batch(10)\n",
        "\n",
        "# set regularization factor\n",
        "reg_fact = 0.01\n",
        "\n",
        "# build model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units=64, activation=tf.keras.activations.relu, kernel_regularizer=tf.keras.regularizers.FIXME(l2=reg_fact), input_shape=[2]))\n",
        "model.add(tf.keras.layers.Dense(units=64, activation=tf.keras.activations.relu, kernel_regularizer=tf.keras.regularizers.FIXME(l2=reg_fact)))\n",
        "model.add(tf.keras.layers.Dense(units=64, activation=tf.keras.activations.relu, kernel_regularizer=tf.keras.regularizers.FIXME(l2=reg_fact)))\n",
        "model.add(tf.keras.layers.Dense(units=1))\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(),\n",
        "              loss=tf.keras.losses.MeanSquaredError())\n",
        "model.summary()\n",
        "\n",
        "# fit model\n",
        "model.fit(train_data, epochs=500, validation_data=valid_data)\n",
        "\n",
        "# predict using fitted model\n",
        "y_hat = model.predict(x)\n",
        "\n",
        "# plot data and model fit\n",
        "fig = plt.figure(figsize=(12,12))\n",
        "axs = fig.add_subplot(projection='3d')\n",
        "axs.scatter(train_x[:,0], train_x[:,1], train_y, marker='o', s=64)\n",
        "axs.scatter(valid_x[:,0], valid_x[:,1], valid_y, marker='x', s=64)\n",
        "axs.scatter(x[:,0], x[:,1], y_hat, marker='+', s=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWW5bn_WAt8k"
      },
      "source": [
        "Make note of the final loss and val_loss values using L2 regularization with a regularization factor of 0.01; you may need these for the associated quiz!\n",
        "\n",
        "After completing the quiz, I encourage you to take some time to play around with L2 regularization factors. How small can you make the regularization factor while still mitigating model overfitting?\n",
        "\n",
        "Of course, you should resist the temptation to directly compare the regularization factors, loss and val_loss values from L1 vs L2 regularization!\n",
        "\n",
        "In general, we would like model regularization to effectively mitigate model overfitting while having a minimal impact on the model's loss (and val_loss)."
      ]
    }
  ]
}
