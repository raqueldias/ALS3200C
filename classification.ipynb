{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNGvBHoJUuF/QhH/7+vQl2/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34LKIju5vzj5"
      },
      "source": [
        "# simulating data for classification\n",
        "\n",
        "To train a neural network for object classification, we need some training data. We've already seen how we can use Scikit-learn to simulate training data for regression problems, but training data for classification problems is different. We need data with one-hot encoded class labels, and it would be nice if the data used for classification was actually correlated with the class labels.\n",
        "\n",
        "Fortunately for us, Scikit-learn has a function, \"make_classification\" that simulates training data for object classification for us! Also fortunately for us, the make_classification function is very similar to the function we used previously for regression problems.\n",
        "\n",
        "The following code cell uses the make_classification function to generate 100 data samples for binary classification. \n",
        "\n",
        "The \"x\" variable holds the data that will be used by the model to infer the class labels, which are typically called \"features\" in object classification. A classification network uses \"feature\" data to infer the \"class label\" of each data sample. \n",
        "\n",
        "The \"y\" variable holds the class labels.\n",
        "\n",
        "In this case, we are simulating data with 10 features (n_features=10) and 2 possible class labels (n_classes=2). As before, we set the random_state option for reproducibility.\n",
        "\n",
        "After simulating the data, we print the simulated features and the class labels, just to have a look at them. We also print the shapes of the feature data and the class labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YQ_sV1IEwkD"
      },
      "source": [
        "import sklearn.datasets\n",
        "\n",
        "x,y = sklearn.datasets.make_classification(n_samples=100,\n",
        "                                           n_features=10,\n",
        "                                           n_classes=2,\n",
        "                                           random_state=77317)\n",
        "\n",
        "print(x)\n",
        "print(y)\n",
        "print(x.shape,y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7_PjrIlJAur"
      },
      "source": [
        "There is quite a bit of feature data in the output; 100 data samples of 10 features each, to be precise. You can see that the features are floating-point numbers.\n",
        "\n",
        "The binary class labels (there are 100 of them, of course, one for each data sample) are either 0 or 1.\n",
        "\n",
        "So, it looks like we have some data we can use for binary classification. Of course, we'll need to package these data into a tensorflow Dataset object to train our network, but we can deal with that later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4U3ymBGExIw"
      },
      "source": [
        "# binary classification\n",
        "\n",
        "To do binary classification, we need a neural network. We are free to build any network we want, but the input and output shapes of the network must match the shape of the features and the shape of the class labels, respectively.\n",
        "\n",
        "Let's look at the data to determine the input and output shapes required.\n",
        "\n",
        "The feature data \"x\" has shape (100,10). There are 100 data samples (we can ignore this, it will be handled by the batch dimension in tensorflow), and there are 10 features per data sample. So, our feature data are 10-dimensional, and we'll need our network to be able to handle input vectors (rank-1 tensors) in 10 dimensions. This can be done by specifying:\n",
        "\n",
        "    input_shape=[10]\n",
        "\n",
        "in the first layer of our network.\n",
        "\n",
        "The class label data \"y\" has shape (100,); it is basically scalar data, which can be treated as a rank-1 tensor (aka, vector) of 1 dimension in tensorflow. This means we will need a single neuron in our network's output layer.\n",
        "\n",
        "For a binary classification problem, we'll need to specify sigmoid activation for the output layer. In tensorflow, sigmoid activation is implemented using a tf.keras.activations.sigmoid object, so we can specify sigmoid activation in tensorflow using the option:\n",
        "\n",
        "    activation=tf.keras.activations.sigmoid\n",
        "\n",
        "when we create the network's output layer.\n",
        "\n",
        "We'll also need to specify cross-entropy loss when we compile our model. Tensorflow implements cross-entropy loss for binary classification using a tf.keras.losses.BinaryCrossentropy object, so we can specify binary cross-entropy loss using the option:\n",
        "\n",
        "    loss=tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "Pretty simple so far. We can use the same optimizer for classification problems as we did for regression problems. And batch training is the same whether the problem is regression or classification.\n",
        "\n",
        "For classification problems, we often want to know how often our model predicts the correct class label during the training process. We can see this information by adding the option:\n",
        "\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
        "\n",
        "when we call model.fit(...). With this option set, tensorflow will report the proportion of training samples correctly classified at each step in the training process. Ideally, we'd like model accuracy to be very close to 1.0 by the end of the training process.\n",
        "\n",
        "Using this information, let's build a simple *linear* classifier for our data in tensorflow.\n",
        "\n",
        "We'll include the data simulation code in the following code cell, just for completeness. We then need to package the data into a tensorflow Dataset object, build our neural network classifier, compile the network using the appropriate loss function, and train the network. In this case, we won't worry about splitting the data into training and validation subsets, although in practice we typically would."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZB-Ic_OE1WA"
      },
      "source": [
        "import sklearn.datasets\n",
        "import tensorflow as tf\n",
        "\n",
        "# simulate training data\n",
        "x,y = sklearn.datasets.make_classification(n_samples=100,\n",
        "                                           n_features=10,\n",
        "                                           n_classes=2,\n",
        "                                           random_state=77317)\n",
        "\n",
        "# package training data into tensorflow Dataset\n",
        "data = tf.data.Dataset.from_tensor_slices((x,y))\n",
        "\n",
        "# create and summarize linear neural network classifier\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units=1, input_shape=[10], activation=tf.keras.activations.sigmoid))\n",
        "model.summary()\n",
        "\n",
        "# compile model with loss function and optimizer\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "# batch data and train model\n",
        "data = data.batch(10)\n",
        "model.fit(data, epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqQy7NxXRRXI"
      },
      "source": [
        "From the model summary (scroll back up through the training output), you can see that the linear classifier has 11 trainable parameters: a weight for each of the 10 input features, plus the bias term.\n",
        "\n",
        "The model's loss is high when training starts, and accuracy is pretty poor, but the loss and accuracy both improve pretty quickly. By the end of the training process, our linear classifier achieves ~0.9 accuracy, indicating that it can correctly classify approximately 90/100 data samples.\n",
        "\n",
        "Not too shabby for a simple linear model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuT93h7yE22e"
      },
      "source": [
        "# simulating multi-label classification data\n",
        "\n",
        "Let's extend our binary classification problem to the case in which there are more than two possible classes.\n",
        "\n",
        "It's relatively easy to use scikit-learn's make_classification function to simulate multi-label classification data; all you need to do is set the n_classes option to the number of classes you'd like to simulate.\n",
        "\n",
        "In the following code cell, edit the FIXME part to simulate 3 possible classes. You'll notice we added an option:\n",
        "\n",
        "    n_clusters_per_class=1\n",
        "\n",
        "This is just to make the math calculations work in scikit-learn for 3-class data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_cAaMuKE59c"
      },
      "source": [
        "import sklearn.datasets\n",
        "\n",
        "x,y = sklearn.datasets.make_classification(n_samples=100,\n",
        "                                           n_features=10,\n",
        "                                           n_classes=FIXME,\n",
        "                                           n_clusters_per_class=1,\n",
        "                                           random_state=77317)\n",
        "\n",
        "print(x)\n",
        "print(y)\n",
        "print(x.shape,y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZVXdUUVg7lk"
      },
      "source": [
        "If everything works out, you should see similar feature data (x) as before. The class label data (y) might look a bit strange, though!\n",
        "\n",
        "Notice that the class labels appear to be integers: 0, 1, 2. These are definitely *not* one-hot encoded!\n",
        "\n",
        "Not to worry, we'll deal with one-hot encoding in tensorflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNvQoaMYE7MI"
      },
      "source": [
        "# multi-label classification\n",
        "\n",
        "Now that we have some 3-class data, we need to build a neural network classifier to classify it.\n",
        "\n",
        "Notice that the shape of the feature data (x) is the same as before, so we can use the same input_shape option for our network's input layer.\n",
        "\n",
        "The class label data (y) needs some special attention. Although the shape of the class label data is the same as before, there are now 3 possible classes, rather than 2. *If* the class label data were one-hot encoded, we'd need 3 output neurons, one for each of the possible classes. The *actual* class label data are *not* one-hot encoded, but we're going to allow tensorflow to internally *convert* the data to one-hot encoding, so we need to design our network's output layer *as if* the data were one-hot encoded. That means we need 3 neurons in the output layer.\n",
        "\n",
        "We'll need to use softmax activation in the output layer, as well. Tensorflow implements softmax activation as the object: tf.keras.activations.softmax, so we can specify:\n",
        "\n",
        "    activation=tf.keras.activations.softmax\n",
        "\n",
        "when we create the output layer, in order to use softmax activation.\n",
        "\n",
        "We'll use the loss function to calculate cross-entropy loss *and* convert the class labels to one-hot encoding, all in one step.\n",
        "\n",
        "To do this, we'll use a tf.keras.losses.SparseCategoricalCrossentropy object. This object automatically converts the integer-encoded class labels (y) to one-hot encoding (\"Sparse\") and calculates multi-label cross-entropy loss (\"CategoricalCrossentropy\"). We'll just need to specify:\n",
        "\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "when we compile the model.\n",
        "\n",
        "Other than those changes, the rest of the model construction and training is pretty much the same. We do have to change the accuracy metric from tf.keras.metrics.BinaryAccuracy to tf.keras.metrics.SparseCategoricalAccuracy, to handle >2 possible classes (see line #22).\n",
        "\n",
        "The following code cell implements multi-label data simulation, builds a model, compiles it, and executes batch training. Edit the FIXME portions to simulate 3-class data (line #7), use softmax activation (line #16) and train using sparse categorical cross-entropy loss (line #21)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCGdoschE-ou"
      },
      "source": [
        "import sklearn.datasets\n",
        "import tensorflow as tf\n",
        "\n",
        "# simulate training data\n",
        "x,y = sklearn.datasets.make_classification(n_samples=100,\n",
        "                                           n_features=10,\n",
        "                                           n_classes=FIXME,\n",
        "                                           n_clusters_per_class=1,\n",
        "                                           random_state=77317)\n",
        "\n",
        "# package training data into tensorflow Dataset\n",
        "data = tf.data.Dataset.from_tensor_slices((x,y))\n",
        "\n",
        "# create and summarize linear neural network classifier\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units=3, input_shape=[10], activation=FIXME))\n",
        "model.summary()\n",
        "\n",
        "# compile model with loss function and optimizer\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(),\n",
        "              loss=FIXME,\n",
        "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "# batch data and train model\n",
        "data = data.batch(10)\n",
        "model.fit(data, epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30l8zpanE_N9"
      },
      "source": [
        "The model now has 33 trainable parameters, 11 for *each* of the 3 output neurons. When I ran the previous code cell, the model's loss stabilized to around 0.44, and the accuracy plateaued at around 0.87. Not too bad.\n",
        "\n",
        "Let's see if we can 'improve' our model's accuracy on the training data by adding a few more layers *before* the output layer.\n",
        "\n",
        "Edit the following code cell to include 2 new Dense neural-network layers, each with 8 units and ReLU activation. Remember that you'll need to specify the input_shape of the *first* layer in the network. Notice that we've also increased the training run to 300 epochs, in order to better fit the model's additional parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p99ZhF1tHskO"
      },
      "source": [
        "import sklearn.datasets\n",
        "import tensorflow as tf\n",
        "\n",
        "# simulate training data\n",
        "x,y = sklearn.datasets.make_classification(n_samples=100,\n",
        "                                           n_features=10,\n",
        "                                           n_classes=FIXME,\n",
        "                                           n_clusters_per_class=1,\n",
        "                                           random_state=77317)\n",
        "\n",
        "# package training data into tensorflow Dataset\n",
        "data = tf.data.Dataset.from_tensor_slices((x,y))\n",
        "\n",
        "# create and summarize linear neural network classifier\n",
        "model = tf.keras.models.Sequential()\n",
        "FIXME\n",
        "model.add(tf.keras.layers.Dense(units=3, activation=FIXME))\n",
        "model.summary()\n",
        "\n",
        "# compile model with loss function and optimizer\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(),\n",
        "              loss=FIXME,\n",
        "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "# batch data and train model\n",
        "data = data.batch(10)\n",
        "model.fit(data, epochs=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7Rutz6nJ9AZ"
      },
      "source": [
        "This model has 187 trainable parameters, more than the number of data samples used to train the model! This should make us highly suspicious that our model might be overfitting the training data!\n",
        "\n",
        "Nonetheless, this model should achieve fairly high accuracy on the training data.\n",
        "\n",
        "After completing the quiz, I'll leave it as a do-on-your-own exercise to:\n",
        "\n",
        "1. simulate a larger data set, and see if your model still achieves high accuracy, and\n",
        "2. implement a train-validate split and assess model overfitting.\n"
      ]
    }
  ]
}
